<!DOCTYPE html>
<html lang="en">
<head>
	<title>Scaling implicit neural representations of images</title>

	<meta name="viewport" content="width=device-width, initial-scale=1">
	<meta name="theme-color" content="#FBEDEA" />
	<meta charset="utf-8">

	<link rel="shortcut icon" type="image/x-icon"  href="../../favicon.ico?">
	<link rel="apple-touch-icon" href="../../apple-touch-icon.png">

	<link rel="stylesheet" href="style.css">

	<script>
	window.MathJax = {
	tex: {
		inlineMath: [['$', '$'], ['\\(', '\\)']]
	}
	};
	</script>

	<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

	<link rel="preconnect" href="https://fonts.googleapis.com">
	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>

	<link href="https://fonts.googleapis.com/css2?family=Roboto+Mono:wght@400;500;600;700&display=swap" rel="stylesheet">

	<link rel="stylesheet" href="../atom-one-dark.min.css">
	<script src="../highlight.min.js"></script>

	<script>hljs.highlightAll();</script>

</head>
<body>
	<div class="blog centering" id="back_container">
		<a href="../index.html" class="back"><svg xmlns="http://www.w3.org/2000/svg" width="18" height="14" fill="currentColor" class="bi bi-caret-left" viewBox="0 0 22 18">
			<path fill-rule="evenodd" clip-rule="evenodd" d="M14.0303 7.46967C14.3232 7.76256 14.3232 8.23744 14.0303 8.53033L10.5607 12L14.0303 15.4697C14.3232 15.7626 14.3232 16.2374 14.0303 16.5303C13.7374 16.8232 13.2626 16.8232 12.9697 16.5303L8.96967 12.5303C8.67678 12.2374 8.67678 11.7626 8.96967 11.4697L12.9697 7.46967C13.2626 7.17678 13.7374 7.17678 14.0303 7.46967Z"/>
		  </svg>All posts</a>
	</div>

	<section class="blog centering post">
		<h1>Scaling implicit neural representations of images</h1>
		<div class="subtitle monospace">By Simon Halvdansson&nbsp;|&nbsp;Dec. 2025</div>

		<hr class="squiggly-line"/>

		<p>
			Implicit neural representations (INRs) aim to encode functions between low dimensional spaces such as audio signals, images, 3D models or videos by means of a neural network. The field is evolving rapidly but is still relatively nascent with some variability in the architectures used. While there has been some <a href="https://arxiv.org/abs/2411.03688?" target="_blank">survey work</a> which is good for an overview, in this post we aim to provide a more comprehensive comparisons between some methods, talk about possible improvements (mixture-of-experts, LayerNorm and skip connections) and provide insights into performance scaling.
		</p>

		<div class="figure_container">
			<a data-fancybox="gallery0" href="media/">
				<img class="figure" src="media/"></img>
			</a>
		</div>
		<div class="figure_caption">
			<span class="caption_title">Figure: </span> TODO.
		</div>

		<p>
			Our findings may be summarized as follows:
		</p>
		<ul>
			<li>
				More intricate activation functions are harder more hyperparameter sensitive but can achieve better performance for small networks. For larger parameter counts, standard MLPs seem to perform better.
			</li>
			<li>
				The standard ReLU activation function appears to be optimal when using feature mappings.
			</li>
			<li>
				MoE, while expensive parameter-wise, can indeed improve performance both at fixed compute budgets and for longer training
			</li>
			<li>
				LayerNorm is very strong for improving convergence speed and training stability when used with a standard MLP 
			</li>
			<li>
				Skip connections do not seem to help either convergence 
			</li>
			<li>
				TODO: Some scaling behavior here
			</li>
		</ul>

		<hr class="squiggly-line"/>

		<h2>The benchmark problem</h2>

		<p>
			We choose to focus fully on INRs of <i>images</i> to reduce the amount of work, meaning that we learn functions $I : [0,1]^2 \to [0,1]^3$ where $(x,y) \mapsto (r,g,b)$. Specifically, let $\mathcal{N}$ be a neural network. We then choose something like mean squared error as our loss function meaning that we minimize
		</p>
		<div class="equation">
			$
			\displaystyle\frac{1}{W\cdot H}\displaystyle\sum_x^W \displaystyle\sum_y^H \big| I(x/W, y/H) - \mathcal{N}(x/W, y/H)\big|^2.
			$
		</div>
		<p>
			To get a comparision which has hopes of generalizing, we choose a collection of 5 images, each of resolution <code>768Ã—768</code> pixels. Our error metric will be the standard <a target="_blank" href="https://en.wikipedia.org/wiki/Peak_signal-to-noise_ratio">peak signal-to-noise ratio</a> (PSNR) since this is often used in papers when comparing models. To keep the comparison fair, we initially train the models for 3 minutes per image. This seems to be in line with or less than the typical training time used in papers.
		</p>

		<div class="figure_container_small" style="padding:0">
			<a data-fancybox="gallery2" href="images/image3.jpg">
				<img class="figure" src="images/image3.jpg"></img>
			</a>
			<a style="display:none" data-fancybox="gallery2" href="images/image2.jpg">
				<img class="figure" src="images/image2.jpg"></img>
			</a>
			<a style="display:none" data-fancybox="gallery2" href="images/image1.jpg">
				<img class="figure" src="images/image1.jpg"></img>
			</a>
			<a style="display:none" data-fancybox="gallery2" href="images/image4.jpg">
				<img class="figure" src="images/image4.jpg"></img>
			</a>
			<a style="display:none" data-fancybox="gallery2" href="images/image5.jpg">
				<img class="figure" src="images/image5.jpg"></img>
			</a>
		</div>
		<div class="figure_caption">
			<span class="caption_title">Figure: </span> The five example images used for the evaluation (click for gallery). Credit: 
			<a target="_blank" href="https://unsplash.com/photos/empty-train-station-platform-at-night-with-rain-8BTrldpHRMY">1</a>
			<a target="_blank" href="https://unsplash.com/photos/abstract-swirling-lines-with-chromatic-aberration-on-black-background-X06-xNC1Md0">2</a>
			<a target="_blank" href="https://unsplash.com/photos/a-set-of-stairs-leading-up-to-a-building-t2lJCJDGRwE">3</a>
			<a target="_blank" href="https://unsplash.com/photos/waves-crash-against-rocky-sea-stacks-under-cloudy-sky-d33ernjpaHk">4</a>
			<a target="_blank" href="https://unsplash.com/photos/tractor-harvesting-crops-in-a-field-DdcWKBbJeEI">5</a>.
		</div>

		<p>
			Before going into the results, we briefly go over what our models look like.
		</p>

		<hr class="squiggly-line"/>

		<h2>INR models</h2>

		<p>
			We will limit ourselves to a more classical set of architectures for our INRs. Specifically, we will only deal with an optional position encoding step followed by an MLP which we allow for some freedom over. In the language of the <a href="https://arxiv.org/abs/2411.03688" target="_blank">INR survey</a> mentioned earlier, this includes classes (a), (b) and (c) from the figure below.
		</p>

		<div class="figure_container">
			<a data-fancybox="gallery1" href="media/inr_types.png">
				<img class="figure" src="media/inr_types.png"></img>
			</a>
		</div>
		<div class="figure_caption">
			<span class="caption_title">Figure: </span> A grouping of INR architectures from <a href="https://arxiv.org/abs/2411.03688" target="_blank">Where Do We Stand with Implicit Neural Representations? A Technical and Performance Survey</a> by Essakine et al.
		</div>

		<p>
			Feature encodings:
		</p>
		<ul>
			<li>Positional encodings (None, Fourier, Gabor)</li>
			<li>Trainable encodings (boolean)</li>
			<li>Frequency scale</li>
			<li>Encoding dimension</li>
		</ul>

		<p>
			MLP
		</p>
		<ul>
			<li>Layer type</li>
			<li>Number of layers</li>
			<li>Hidden dimension</li>
			<li>Number of expers</li>
			<li>Skip connections</li>
			<li>LayerNorm</li>
		</ul>

		<p>
			General
		</p>
		<ul>
			<li>Learning rate</li>
			<li>Batch size</li>
		</ul>

	</section>
	
	<link rel="stylesheet" href="../fancybox.css" />
	<script src="../fancybox.umd.js"></script>

	<script>
		Fancybox.bind("[data-fancybox]", {
			closeButton: false,
		});
	  </script>


</body>
</html>