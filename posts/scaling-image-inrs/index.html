<!DOCTYPE html>
<html lang="en">
<head>
	<title>Scaling implicit neural representations of images</title>

	<meta name="viewport" content="width=device-width, initial-scale=1">
	<meta name="theme-color" content="#FBEDEA" />
	<meta charset="utf-8">

	<link rel="shortcut icon" type="image/x-icon"  href="../../favicon.ico?">
	<link rel="apple-touch-icon" href="../../apple-touch-icon.png">

	<link rel="stylesheet" href="style.css">

	<script>
	window.MathJax = {
	tex: {
		inlineMath: [['$', '$'], ['\\(', '\\)']]
	}
	};
	</script>

	<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

	<link rel="preconnect" href="https://fonts.googleapis.com">
	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>

	<link href="https://fonts.googleapis.com/css2?family=Roboto+Mono:wght@400;500;600;700&display=swap" rel="stylesheet">

	<link rel="stylesheet" href="../atom-one-dark.min.css">
	<script src="../highlight.min.js"></script>

	<script>hljs.highlightAll();</script>

</head>
<body>
	<div class="blog centering" id="back_container">
		<a href="../index.html" class="back"><svg xmlns="http://www.w3.org/2000/svg" width="18" height="14" fill="currentColor" class="bi bi-caret-left" viewBox="0 0 22 18">
			<path fill-rule="evenodd" clip-rule="evenodd" d="M14.0303 7.46967C14.3232 7.76256 14.3232 8.23744 14.0303 8.53033L10.5607 12L14.0303 15.4697C14.3232 15.7626 14.3232 16.2374 14.0303 16.5303C13.7374 16.8232 13.2626 16.8232 12.9697 16.5303L8.96967 12.5303C8.67678 12.2374 8.67678 11.7626 8.96967 11.4697L12.9697 7.46967C13.2626 7.17678 13.7374 7.17678 14.0303 7.46967Z"/>
		  </svg>All posts</a>
	</div>

	<section class="blog centering post">
		<h1>Scaling implicit neural representations of images</h1>
		<div class="subtitle monospace">By Simon Halvdansson&nbsp;|&nbsp;Dec. 2025</div>

		<hr class="squiggly-line"/>

		<p>
			Implicit neural representations (INRs) aim to encode functions by means of a neural network. The field is evolving rapidly and is still relatively nascent with no clear concensus on the proper architecture to use. Comparing methods in papers, ablations studies often leave a lot to wish for. In this post, we compare several methods to try and determine what a good architecture is for an INR of images and how this changes as we increase the number of parameters and runtime of the models.
		</p>

		<div class="figure_container">
			<a data-fancybox="gallery0" href="media/">
				<img class="figure" src="media/"></img>
			</a>
		</div>
		<div class="figure_caption">
			<span class="caption_title">Figure: </span> TODO.
		</div>

		<p>
			To make this a proper investigation, we need to give 
		</p>

		<hr class="squiggly-line"/>

		<h2>The benchmark problem</h2>

		<p>
			We choose to focus fully on INRs of <i>images</i>, meaning that we learn functions $I : [0,1]^2 \to [0,1]^3$ where $(x,y) \mapsto (r,g,b)$. Specifically, let $\mathcal{N}$ be a neural network. We then choose something like mean squared error as our loss function meaning that we minimize
		</p>
		<div class="equation">
			$
			\displaystyle\frac{1}{W\cdot H}\displaystyle\sum_{0 \leq x \leq W} \displaystyle\sum_{0 \leq y \leq H} \big| I(x/W, y/H) - \mathcal{N}(x/W, y/H)\big|^2.
			$
		</div>
		<p>
			For a fair comparision which has hopes of generalizing, we choose a collection of 5 images, each of resolution <code>768Ã—768</code> pixels. Our error metric will be the standard <a target="_blank" href="https://en.wikipedia.org/wiki/Peak_signal-to-noise_ratio">peak signal-to-noise ratio</a> (PSNR) since this is often used in papers when comparing models.
		</p>


		<hr class="squiggly-line"/>

		<h2>INR models</h2>

		<p>
			There are some more advanced but we will focus on MLP-based algos + MoE.
		</p>
		

	</section>
	
	<link rel="stylesheet" href="../fancybox.css" />
	<script src="../fancybox.umd.js"></script>


	<script>
		Fancybox.bind("[data-fancybox]", {
			closeButton: false,
		});
	  </script>


</body>
</html>