<!DOCTYPE html>
<html lang="en">
<head>
	<title>Scaling implicit neural representations of images</title>

	<meta name="viewport" content="width=device-width, initial-scale=1">
	<meta name="theme-color" content="#FBEDEA" />
	<meta charset="utf-8">

	<link rel="shortcut icon" type="image/x-icon"  href="../../favicon.ico?">
	<link rel="apple-touch-icon" href="../../apple-touch-icon.png">

	<link rel="stylesheet" href="style.css">

	<script>
	window.MathJax = {
	tex: {
		inlineMath: [['$', '$'], ['\\(', '\\)']]
	}
	};
	</script>

	<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

	<link rel="preconnect" href="https://fonts.googleapis.com">
	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>

	<link href="https://fonts.googleapis.com/css2?family=Roboto+Mono:wght@400;500;600;700&display=swap" rel="stylesheet">

	<link rel="stylesheet" href="../atom-one-dark.min.css">
	<script src="../highlight.min.js"></script>

	<script>hljs.highlightAll();</script>

</head>
<body>
	<div class="blog centering" id="back_container">
		<a href="../index.html" class="back"><svg xmlns="http://www.w3.org/2000/svg" width="18" height="14" fill="currentColor" class="bi bi-caret-left" viewBox="0 0 22 18">
			<path fill-rule="evenodd" clip-rule="evenodd" d="M14.0303 7.46967C14.3232 7.76256 14.3232 8.23744 14.0303 8.53033L10.5607 12L14.0303 15.4697C14.3232 15.7626 14.3232 16.2374 14.0303 16.5303C13.7374 16.8232 13.2626 16.8232 12.9697 16.5303L8.96967 12.5303C8.67678 12.2374 8.67678 11.7626 8.96967 11.4697L12.9697 7.46967C13.2626 7.17678 13.7374 7.17678 14.0303 7.46967Z"/>
		  </svg>All posts</a>
	</div>

	<section class="blog centering post">
		<h1>Scaling implicit neural representations of images</h1>
		<div class="subtitle monospace">By Simon Halvdansson&nbsp;|&nbsp;Dec. 2025</div>

		<hr class="squiggly-line"/>

		<p>
			Implicit neural representations (INRs) aim to encode functions between low dimensional spaces such as audio signals, images, 3D models or videos by means of a neural network. The field is evolving rapidly but is still relatively nascent with some variability in the architectures used. While there has been some <a href="https://arxiv.org/abs/2411.03688?" target="_blank">survey work</a> which is good for an overview, in this post we aim to provide a more comprehensive comparisons between some methods, talk about possible improvements (mixture-of-experts, LayerNorm and skip connections) and provide insights into performance scaling.
		</p>

		<div class="figure_container">
			<a data-fancybox="gallery0" href="media/">
				<img class="figure" src="media/"></img>
			</a>
		</div>
		<div class="figure_caption">
			<span class="caption_title">Figure: </span> TODO.
		</div>

		<p>
			Our findings may be summarized as follows:
		</p>
		<ul>
			<li>
				More intricate activation functions are more hyperparameter sensitive but can achieve better performance for small networks. For larger parameter counts, standard MLPs seem to perform better.
			</li>
			<li>
				The standard ReLU activation function appears to be optimal when using feature mappings.
			</li>
			<li>
				MoE, while expensive parameter-wise, can indeed improve performance both at fixed compute budgets and for longer training
			</li>
			<li>
				LayerNorm is very strong for improving convergence speed and training stability when used with a standard MLP 
			</li>
			<li>
				Skip connections do not seem to help either convergence 
			</li>
			<li>
				TODO: Some scaling behavior here
			</li>
		</ul>

		<hr class="squiggly-line"/>

		<h2>The benchmark problem</h2>

		<p>
			We choose to focus fully on INRs of <i>images</i> to reduce the amount of work, meaning that we learn functions $I : [0,1]^2 \to [0,1]^3$ where $(x,y) \mapsto (r,g,b)$. Specifically, let $\mathcal{N}$ be a neural network. We then choose something like mean squared error as our loss function meaning that we minimize
		</p>
		<div class="equation">
			$
			\displaystyle\frac{1}{W\cdot H}\displaystyle\sum_x^W \displaystyle\sum_y^H \big| I(x/W, y/H) - \mathcal{N}(x/W, y/H)\big|^2.
			$
		</div>
		<p>
			To get a comparision which has hopes of generalizing, we choose a collection of 5 images, each of resolution <code>768×768</code> pixels. Our error metric will be the standard <a target="_blank" href="https://en.wikipedia.org/wiki/Peak_signal-to-noise_ratio">peak signal-to-noise ratio</a> (PSNR) since this is often used in papers when comparing models. To keep the comparison fair, we initially train the models for 3 minutes per image. This seems to be in line with or less than the typical training time used in papers.
		</p>

		<div class="figure_container_small" style="padding:0">
			<a data-fancybox="gallery2" href="images/station_min.jpg">
				<img class="figure" src="images/station_min.jpg"></img>
			</a>
			<a style="display:none" data-fancybox="gallery2" href="images/farming_min.jpg">
				<img class="figure" src="images/farming_min.jpg"></img>
			</a>
			<a style="display:none" data-fancybox="gallery2" href="images/stairs_min.jpg">
				<img class="figure" src="images/stairs_min.jpg"></img>
			</a>
			<a style="display:none" data-fancybox="gallery2" href="images/thing_min.jpg">
				<img class="figure" src="images/thing_min.jpg"></img>
			</a>
			<a style="display:none" data-fancybox="gallery2" href="images/waves_min.jpg">
				<img class="figure" src="images/waves_min.jpg"></img>
			</a>
		</div>
		<div class="figure_caption">
			<span class="caption_title">Figure: </span> The five example images used for the evaluation (click for gallery). Credit: 
			<a target="_blank" href="https://unsplash.com/photos/empty-train-station-platform-at-night-with-rain-8BTrldpHRMY">1</a>
			<a target="_blank" href="https://unsplash.com/photos/abstract-swirling-lines-with-chromatic-aberration-on-black-background-X06-xNC1Md0">2</a>
			<a target="_blank" href="https://unsplash.com/photos/a-set-of-stairs-leading-up-to-a-building-t2lJCJDGRwE">3</a>
			<a target="_blank" href="https://unsplash.com/photos/waves-crash-against-rocky-sea-stacks-under-cloudy-sky-d33ernjpaHk">4</a>
			<a target="_blank" href="https://unsplash.com/photos/tractor-harvesting-crops-in-a-field-DdcWKBbJeEI">5</a>.
		</div>

		<p>
			Before going into the results, we briefly go over what our models look like.
		</p>

		<hr class="squiggly-line"/>

		<h2>INR models</h2>

		<p>
			We will limit ourselves to a more classical set of architectures for our INRs. Specifically, we will only deal with an optional position encoding step followed by an MLP which we allow for some freedom over. In the language of the <a href="https://arxiv.org/abs/2411.03688" target="_blank">INR survey</a> mentioned earlier, this includes classes (a), (b) and (c) from the figure below.
		</p>

		<div class="figure_container">
			<a data-fancybox="gallery1" href="media/for_post/inr_types.png">
				<img class="figure" src="media/for_post/inr_types.png"></img>
			</a>
		</div>
		<div class="figure_caption">
			<span class="caption_title">Figure: </span> A grouping of INR architectures from <a href="https://arxiv.org/abs/2411.03688" target="_blank">Where Do We Stand with Implicit Neural Representations? A Technical and Performance Survey</a> by Essakine et al.
		</div>

		<h4>Position encoding</h4>
		
		<p>
			Formally, our neural network then has the form $\mathcal{N}(x) = \operatorname{MLP}(\gamma(x))$. For the feature encoding $\gamma$ we will consider Fourier features and Gabor features, discussed in an <a href="../gabor-feature-experiments/index.html">earlier post</a>. In PyTorch we write these as follows:
		</p>

		<pre><code class="language-py">class FourierFeatures(nn.Module):
    def __init__(self, input_dim, dim_per_input=20, freq_scale=None, trainable=False):
        super().__init__()

        if freq_scale is None:
            freq_scale = 0.5*dim_per_input/torch.sqrt(torch.tensor(2 * torch.pi))
        
        freqs = torch.randn(input_dim, dim_per_input) * freq_scale

        if trainable:
            self.freqs = nn.Parameter(freqs)
        else:
            self.register_buffer('freqs', freqs)

    def forward(self, x):
        phase = 2 * torch.pi * (x @ self.freqs)
        return torch.cat([torch.cos(phase), torch.sin(phase)], dim=-1)
			
class GaborFeatures(nn.Module):
    def __init__(self, input_dim, dim_per_input=20, freq_scale=None, trainable=False):
        super().__init__()
        
        if freq_scale is None:
            freq_scale = 0.5*dim_per_input/torch.sqrt(torch.tensor(2 * torch.pi))

        centers = torch.rand(dim_per_input, input_dim)
        freqs = torch.randn(dim_per_input, input_dim) * freq_scale
        sigmas = torch.full((dim_per_input, input_dim), 0.1*torch.sqrt(torch.tensor(256/input_dim)))

        if trainable:
            self.centers = nn.Parameter(centers)
            self.freqs = nn.Parameter(freqs)
            self.sigma = nn.Parameter(sigmas)
        else:
            self.register_buffer('centers', centers)
            self.register_buffer('frequencies', freqs)
            self.register_buffer('sigma', sigmas)

    def forward(self, x):
        diff = x.unsqueeze(1) - self.centers.unsqueeze(0)
        envelope = torch.exp(- (diff ** 2 / (self.sigma.unsqueeze(0) ** 2)).sum(dim=2) / 2)
        phase = 2 * torch.pi * (diff * self.freqs.unsqueeze(0)).sum(dim=2)

        return torch.cat([envelope * torch.cos(phase), envelope * torch.sin(phase)], dim=-1)
</code></pre>

		<p>
			Note that when using position encodings, we have here exposed the parameters <code>trainable</code> for if the frequencies, centers and widths should be viewed as parameters or fixed but random parameters, <code>freq_scale</code> for how we sample the frequencies and <code>dim_per_input</code> which yields the output dimension <code>input_dim × dim_per_input</code>. This opens up a lot of room for optimization.
		</p>

		<h4>MLP</h4>
	
		<p>
			For the MLP backbone, we expose standard hyperparameters like activation function, number of layers and hidden dimension (which we take to be fixed across each layer for simplicity). However we also allow for the three less standard extensions mentioned in the introduction, namely mixture-of-experts (MoE), skip connections and LayerNorm.
		</p>

		<p>
			For the MoE, we pass the raw coordinates through a small gating network with an output of length <code>n_experts</code> which is softmaxed. Then <code>n_experts</code> MLPs treat the positional encoding output to produce pixel values and the results are weighted by the gating outputs before being sigmoided:
		</p>

				<pre><code class="language-py">def forward(self, x):
	gate_logits = self.gate(x)

	x = self.pos_enc(x)

	gate_weights = F.softmax(gate_logits, dim=-1)

	expert_outputs = []
	for expert in self.experts:
		y = expert(x)
		expert_outputs.append(y.unsqueeze(-2))

	expert_outputs = torch.cat(expert_outputs, dim=-2)

	mixed = torch.sum(
		gate_weights.unsqueeze(-1) * expert_outputs,
		dim=-2
	)

	return self.sigmoid(mixed)
</code></pre>

		<p>
			Skip connections are handled inside the MLPs and only applied to the states of length <code>hidden_dim</code> with $\frac{1}{\sqrt{2}}$ weighing, i.e., <code>x = (x + x_skip)/torch.sqrt(2)</code>. For LayerNorm preliminary investigations showed that post vs pre-norm (norm before or after activation function) had a minimal effect on performance and the same was true for <code>LayerNorm</code> versus <code>RMSNorm</code> so we keep it simple and only consider post-LayerNorm. 
		</p>

		<p>
			Lastly we also look at alternative MLP activation functions, including <a  href="https://arxiv.org/abs/2006.09661" target="_blank">SIREN</a> and <a href="https://arxiv.org/abs/2301.05187" target="_blank">WIRE</a>. 
		</p>

						<pre><code class="language-py">class SIRENLayer(nn.Module):
    def __init__(self, in_features, out_features, is_first=False, omega_0=60):
        super().__init__()
        self.omega_0 = omega_0
        self.is_first = is_first
        self.linear = nn.Linear(in_features, out_features)

        self.init_weights()

    def init_weights(self):
        with torch.no_grad():
            if self.is_first:
                self.linear.weight.uniform_(-1 / self.linear.in_features,
                                             1 / self.linear.in_features)
            else:
                self.linear.weight.uniform_(-torch.sqrt(torch.tensor(6) / self.linear.in_features) / self.omega_0,
                                             torch.sqrt(torch.tensor(6) / self.linear.in_features) / self.omega_0)

    def forward(self, x):
        x = self.linear(x)
        return torch.sin(self.omega_0 * x)

class WIRELayer(nn.Module):
    def __init__(self, in_features, out_features, s0, w0):
        super().__init__()
        self.linear = nn.Linear(in_features, out_features)
        self.register_buffer("w0", torch.tensor(w0))
        self.register_buffer("s0", torch.tensor(s0))

    def forward(self, x):
        x = self.linear(x)
        return torch.exp(-(self.s0 * x) ** 2) * torch.sin(self.w0 * x)
</code></pre>

		<p>
			Obviously we will not want to use position encodings at the same time as these alternative MLP layers since they are made to replace them.
		</p>

		<h4>Training hyperparameters</h4>

		<p>
			We will also look at tweaking the learning rate <code>lr</code> and batch size <code>batch_size</code> for the training.
		</p>

		<hr class="squiggly-line"/>

		<h2>Baseline</h2>

		<p>
			First up we train a bog standard INR with Fourier features, an embedding dimension of <code>64</code> and <code>4</code> hidden layers of dimension <code>256</code> for a <code>150k</code> parameter model. Below are snapshots from training that network for 3 minutes per image with a learning rate of <code>1e-4</code>.
		</p>

		<div class="figure_container">
			<a data-fancybox="gallery0" href="media/for_post/training_combined_station__farming__stairs__thing__waves.mp4">
				<video class="figure" src="media/for_post/training_combined_station__farming__stairs__thing__waves.mp4" autoplay muted playsinline loop preload="metadata"></video>
			</a>
		</div>
		<div class="figure_caption">
			<span class="caption_title">Figure: </span> Training snapshots for five different INRs, one for each image, during training with highlighted MSE and PSNR at each frame.
		</div>

		<p>
			In this example the loss is extremely rapidly decreasing in the start and plateus at rather high level. However the level at which the loss curves plateu differ greatly for the different images. We see for example that the farming image is hard to learn, most likely due to its significant high frequency components.
		</p>

		<div class="figure_container_small">
			<a data-fancybox="gallery0" href="media/for_post/loss_curve_combined.png">
				<img class="figure" src="media/for_post/loss_curve_combined.png"></img>
			</a>
		</div>
		<div class="figure_caption">
			<span class="caption_title">Figure: </span> Loss curve for the stairs image in log scale.
		</div>

		<p>
			We now move to improving this performance by using the average PSNR across these five images as our objective function.
		</p>
		
		<hr class="squiggly-line"/>

		<h2>Scaling up standard MLPs</h2>

		<p>
			Our first step will be to try out a larger MLP since performance seems to saturate rather quickly.
		</p>

		<div class="figure_container">
			<a style="float: left; width: 31%;" data-fancybox="gallery0" href="media/for_post/sweep_hidden_dim_vs_n_layers_mse_combined.png">
				<img class="figure" src="media/for_post/sweep_hidden_dim_vs_n_layers_mse_combined.png"></img>
			</a>
			<a style="float: left; width: 31%;" data-fancybox="gallery0" href="media/for_post/sweep_hidden_dim_vs_n_layers_psnr_combined.png">
				<img class="figure" src="media/for_post/sweep_hidden_dim_vs_n_layers_psnr_combined.png"></img>
			</a>
			<a style="float: left; width: 38%;" data-fancybox="gallery0" href="media/for_post/loss_curve_combined_v2.png">
				<img class="figure" src="media/for_post/loss_curve_combined_v2.png"></img>
			</a>
		</div>
		<div class="figure_caption">
			<span class="caption_title">Figure: </span> MSE and PSNR results for MLPs with different <code>hidden_dim</code> (x-axis) and <code>n_layers</code> (y-axis). The loss curves are with the optimal choice.
		</div>

		<p>
			Based on this information we set <code>n_layers = 6</code> and <code>hidden_dim = 1024</code> but leave a note about trying out larger values for these later. The model now has <code>4.3M</code> parameters but we remark that we have chosen to only use training time as our restriction. Obviously this is not the correct restriction if one is attempting to, e.g., compress images using an INR, in which case we would probably have a fixed parameter budget as well.  
		</p>

		<p>
			We could definitively have pushed MLP size further at this point but remark that when we have no positional encodings, we get stuck in this sort of plateu where we can get performance gains from increasing the parameter count but we are fundamentally unable to express the details of the image with and are only digging deeper into a local minima. For this reason we next up look at encoding dimension and frequency scale. Recall that the encoding dimension is the output dimension of $\gamma$ and the frequency scale is the standard deviation of the Gaussian which the frequencies are taken from.
		</p>

		<div class="figure_container_small">
			<a style="float: left; width: 50%;" data-fancybox="gallery0" href="media/for_post/sweep_freq_scale_vs_encoding_dim_mse_combined.png">
				<img class="figure" src="media/for_post/sweep_freq_scale_vs_encoding_dim_mse_combined.png"></img>
			</a>
			<a style="float: left; width: 50%;" data-fancybox="gallery0" href="media/for_post/sweep_freq_scale_vs_encoding_dim_psnr_combined.png">
				<img class="figure" src="media/for_post/sweep_freq_scale_vs_encoding_dim_psnr_combined.png"></img>
			</a>
		</div>
		<div class="figure_caption">
			<span class="caption_title">Figure: </span> MSE and PSNR results for INRs with different <code>encoding_dim</code> (x-axis) and <code>freq_scale</code> (y-axis).
		</div>

		<p>
			Here we get a large jump in average PSNR from <code>26 dB</code> to <code>35 dB</code>. Setting <code>encoding_dim = 256</code> and <code>freq_scale = 80</code>, we now move to tuning the learning rate.
		</p>

		<div class="figure_container_small">
			<a style="float: left; width: 50%;" data-fancybox="gallery0" href="media/for_post/">
				<img class="figure" src="media/for_post/"></img>
			</a>
			<a style="float: left; width: 50%;" data-fancybox="gallery0" href="media/for_post/">
				<img class="figure" src="media/for_post/"></img>
			</a>
		</div>

	</section>
	
	<link rel="stylesheet" href="../fancybox.css" />
	<script src="../fancybox.umd.js"></script>

	<script>
		Fancybox.bind("[data-fancybox]", {
			closeButton: false,
		});
	  </script>

</body>
</html>
