<!DOCTYPE html>
<html lang="en">
<head>
	<title>Implicit neural representations of images are quick learners</title>

	<meta name="viewport" content="width=device-width, initial-scale=1">
	<meta name="theme-color" content="#FBEDEA" />
	<meta charset="utf-8">

	<link rel="shortcut icon" type="image/x-icon"  href="../../favicon.ico?">
	<link rel="apple-touch-icon" href="../../apple-touch-icon.png">

	<link rel="stylesheet" href="style.css">

	<script>
	window.MathJax = {
	tex: {
		inlineMath: [['$', '$'], ['\\(', '\\)']]
	}
	};
	</script>

	<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

	<link rel="preconnect" href="https://fonts.googleapis.com">
	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>

	<link href="https://fonts.googleapis.com/css2?family=Roboto+Mono:wght@400;500;600;700&display=swap" rel="stylesheet">

	<link rel="stylesheet" href="../atom-one-dark.min.css">
	<script src="../highlight.min.js"></script>

	<script>hljs.highlightAll();</script>

</head>
<body>
	<div class="blog centering" id="back_container">
		<a href="../index.html" class="back"><svg xmlns="http://www.w3.org/2000/svg" width="18" height="14" fill="currentColor" class="bi bi-caret-left" viewBox="0 0 22 18">
			<path fill-rule="evenodd" clip-rule="evenodd" d="M14.0303 7.46967C14.3232 7.76256 14.3232 8.23744 14.0303 8.53033L10.5607 12L14.0303 15.4697C14.3232 15.7626 14.3232 16.2374 14.0303 16.5303C13.7374 16.8232 13.2626 16.8232 12.9697 16.5303L8.96967 12.5303C8.67678 12.2374 8.67678 11.7626 8.96967 11.4697L12.9697 7.46967C13.2626 7.17678 13.7374 7.17678 14.0303 7.46967Z"/>
		  </svg>All posts</a>
	</div>

	<section class="blog centering post">
		<h1>Implicit neural representations of images are quick learners</h1>
		<div class="subtitle monospace">By Simon Halvdansson&nbsp;|&nbsp;Dec. 2025</div>

		<hr class="squiggly-line"/>

		<p>
			Implicit neural representations (INRs) aim to encode functions between low dimensional spaces such as audio signals, images, 3D models or videos by means of a neural network. The field is evolving rapidly but is still relatively nascent with some variability in the architectures used. While there has been some <a href="https://arxiv.org/abs/2411.03688?" target="_blank">survey work</a> which is good for an overview, in this post we  provide a deeper investigation about how to set up the MLPs and talk about possible improvements (mixture-of-experts, LayerNorm and skip connections). We find that with some good choices it is possible to learn an image with an INR in mere seconds with excellent performance and talk about some of the implications of this. 
		</p>

		<div class="figure_container">
			<a data-fancybox="galleryvidv3" href="media/for_post/training_combined_station__farming__stairs__thing__waves_v3.mp4">
				<video class="figure" src="media/for_post/training_combined_station__farming__stairs__thing__waves_v3.mp4" autoplay muted playsinline loop preload="metadata"></video>
			</a>
		</div>
		<div class="figure_caption">
			<span class="caption_title">Figure: </span> The training procedure of our best model with MSE and PSNR highlighted for each epoch. Note the high performance and low epoch numbers.
		</div>

		<p>
			Our findings may be summarized as follows:
		</p>
		<ul>
			<li>
				With enough parameters, an INR can learn an image with negligibly small error in seconds. 
			</li>
			<li>
				The standard ReLU activation function appears to be optimal when using feature mappings.
			</li>
			<li>
				Hyperparameters like learning rate and optimizer can have very different behavior for different images.
			</li>
			<li>
				MoE, while expensive parameter-wise, can indeed improve performance both at fixed compute budgets and for longer training
			</li>
			<li>
				LayerNorm is very strong for improving convergence speed and training stability when used with a standard MLP 
			</li>
			<li>
				Skip connections do not seem to help either convergence 
			</li>
		</ul>

		<hr class="squiggly-line"/>

		<h2>The benchmark problem</h2>

		<p>
			We choose to focus fully on INRs of <i>images</i> to reduce the amount of work, meaning that we learn functions $I : [0,1]^2 \to [0,1]^3$ where $(x,y) \mapsto (r,g,b)$. Specifically, let $\mathcal{N}$ be a neural network. We then choose something like mean squared error as our loss function meaning that we minimize
		</p>
		<div class="equation">
			$
			\displaystyle\frac{1}{W\cdot H}\displaystyle\sum_x^W \displaystyle\sum_y^H \big| I(x/W, y/H) - \mathcal{N}(x/W, y/H)\big|^2.
			$
		</div>
		<p>
			To get a comparison which has hopes of generalizing, we choose a collection of 5 images, each of resolution <code>768×768</code> pixels. Our error metric will be the standard <a target="_blank" href="https://en.wikipedia.org/wiki/Peak_signal-to-noise_ratio">peak signal-to-noise ratio</a> (PSNR) since this is often used in papers when comparing models. To keep the comparison fair, we train the models for 3 minutes per image. This seems to be in line with or less than the typical training time used in papers.
		</p>

		<div class="figure_container_small" style="padding:0">
			<a data-fancybox="gallery2" href="images/station_min.jpg">
				<img class="figure" src="images/station_min.jpg"></img>
			</a>
			<a style="display:none" data-fancybox="gallery2" href="images/farming_min.jpg">
				<img class="figure" src="images/farming_min.jpg"></img>
			</a>
			<a style="display:none" data-fancybox="gallery2" href="images/stairs_min.jpg">
				<img class="figure" src="images/stairs_min.jpg"></img>
			</a>
			<a style="display:none" data-fancybox="gallery2" href="images/thing_min.jpg">
				<img class="figure" src="images/thing_min.jpg"></img>
			</a>
			<a style="display:none" data-fancybox="gallery2" href="images/waves_min.jpg">
				<img class="figure" src="images/waves_min.jpg"></img>
			</a>
		</div>
		<div class="figure_caption">
			<span class="caption_title">Figure: </span> The five example images used for the evaluation (click for gallery). Credit: 
			<a target="_blank" href="https://unsplash.com/photos/empty-train-station-platform-at-night-with-rain-8BTrldpHRMY">1</a>
			<a target="_blank" href="https://unsplash.com/photos/abstract-swirling-lines-with-chromatic-aberration-on-black-background-X06-xNC1Md0">2</a>
			<a target="_blank" href="https://unsplash.com/photos/a-set-of-stairs-leading-up-to-a-building-t2lJCJDGRwE">3</a>
			<a target="_blank" href="https://unsplash.com/photos/waves-crash-against-rocky-sea-stacks-under-cloudy-sky-d33ernjpaHk">4</a>
			<a target="_blank" href="https://unsplash.com/photos/tractor-harvesting-crops-in-a-field-DdcWKBbJeEI">5</a>.
		</div>

		<p>
			This is the problem we will hill climb, and we leave questions about its suitability and the implications to the end. Before going into the results, we briefly go over what our models look like.
		</p>

		<hr class="squiggly-line"/>

		<h2>INR models</h2>

		<p>
			We will limit ourselves to a more classical set of architectures for our INRs. Specifically, we will only deal with a position encoding step followed by an MLP which we allow for some freedom over. In the language of the <a href="https://arxiv.org/abs/2411.03688" target="_blank">INR survey</a> mentioned earlier, this means that we modify the yellow parts of (c) in the figure below.
		</p>

		<div class="figure_container">
			<a data-fancybox="gallery_inr" href="media/for_post/inr_types.png">
				<img class="figure" src="media/for_post/inr_types.png"></img>
			</a>
		</div>
		<div class="figure_caption">
			<span class="caption_title">Figure: </span> A grouping of INR architectures from <a href="https://arxiv.org/abs/2411.03688" target="_blank">Where Do We Stand with Implicit Neural Representations? A Technical and Performance Survey</a> by Essakine et al.
		</div>

		<h4>Position encoding</h4>
		
		<p>
			Formally, our neural network then has the form $\mathcal{N}(x) = \operatorname{MLP}(\gamma(x))$. For the feature encoding $\gamma$ we will consider Fourier features and Gabor features, discussed in an <a href="../gabor-feature-experiments/index.html">earlier post</a>. In PyTorch we write these as follows:
		</p>

		<pre><code class="language-py">class FourierFeatures(nn.Module):
    def __init__(self, input_dim, dim_per_input=20, freq_scale=None, trainable=False):
        super().__init__()

        if freq_scale is None:
            freq_scale = 0.5*dim_per_input/torch.sqrt(torch.tensor(2 * torch.pi))
        
        freqs = torch.randn(input_dim, dim_per_input) * freq_scale

        if trainable:
            self.freqs = nn.Parameter(freqs)
        else:
            self.register_buffer('freqs', freqs)

    def forward(self, x):
        phase = 2 * torch.pi * (x @ self.freqs)
        return torch.cat([torch.cos(phase), torch.sin(phase)], dim=-1)
			
class GaborFeatures(nn.Module):
    def __init__(self, input_dim, dim_per_input=20, freq_scale=None, trainable=False):
        super().__init__()
        
        if freq_scale is None:
            freq_scale = 0.5*dim_per_input/torch.sqrt(torch.tensor(2 * torch.pi))

        centers = torch.rand(dim_per_input, input_dim)
        freqs = torch.randn(dim_per_input, input_dim) * freq_scale
        sigmas = torch.full((dim_per_input, input_dim), 0.1*torch.sqrt(torch.tensor(256/input_dim)))

        if trainable:
            self.centers = nn.Parameter(centers)
            self.freqs = nn.Parameter(freqs)
            self.sigma = nn.Parameter(sigmas)
        else:
            self.register_buffer('centers', centers)
            self.register_buffer('freqs', freqs)
            self.register_buffer('sigma', sigmas)

    def forward(self, x):
        diff = x.unsqueeze(1) - self.centers.unsqueeze(0)
        envelope = torch.exp(- (diff ** 2 / (self.sigma.unsqueeze(0) ** 2)).sum(dim=2) / 2)
        phase = 2 * torch.pi * (diff * self.freqs.unsqueeze(0)).sum(dim=2)

        return torch.cat([envelope * torch.cos(phase), envelope * torch.sin(phase)], dim=-1)
</code></pre>

		<p>
			Note that when using position encodings, we have here exposed the parameter <code>trainable</code> for if the frequencies, centers and widths should be viewed as parameters or fixed but random parameters, <code>freq_scale</code> for how we sample the frequencies and <code>dim_per_input</code> which yields the output dimension <code>input_dim × dim_per_input</code>. This opens up a lot of room for optimization.
		</p>

		<h4>MLP</h4>
	
		<p>
			For the MLP backbone, we expose standard hyperparameters like activation function, number of layers and hidden dimension (which we take to be fixed across each layer for simplicity). However we also allow for the three less standard extensions mentioned in the introduction, namely mixture-of-experts (MoE), skip connections and LayerNorm.
		</p>

		<p>
			For the MoE, we pass the raw coordinates through a small gating network with an output of length <code>n_experts</code> which is softmaxed. Then <code>n_experts</code> MLPs treat the positional encoding output to produce pixel values and the results are weighted by the gating outputs before being sigmoided:
		</p>

				<pre><code class="language-py">def forward(self, x):
	gate_logits = self.gate(x)

	x = self.pos_enc(x)

	gate_weights = F.softmax(gate_logits, dim=-1)

	expert_outputs = []
	for expert in self.experts:
		y = expert(x)
		expert_outputs.append(y.unsqueeze(-2))

	expert_outputs = torch.cat(expert_outputs, dim=-2)

	mixed = torch.sum(
		gate_weights.unsqueeze(-1) * expert_outputs,
		dim=-2
	)

	return self.sigmoid(mixed)
</code></pre>

		<p>
			Skip connections are handled inside the MLPs and only applied to the states of length <code>hidden_dim</code> with $\frac{1}{\sqrt{2}}$ weighing, i.e., <code>x = (x + x_skip)/torch.sqrt(2)</code>. For LayerNorm preliminary investigations showed that post vs pre-norm (norm before or after activation function) had a minimal effect on performance and the same was true for <code>LayerNorm</code> versus <code>RMSNorm</code> so we keep it simple and only consider post-LayerNorm. 
		</p>

		<h4>Training hyperparameters</h4>

		<p>
			We will also look at tweaking the learning rate <code>lr</code> and batch size <code>batch_size</code> for the training.
		</p>

		<hr class="squiggly-line"/>

		<h2>Baseline</h2>

		<p>
			First up we train a bog standard INR with Fourier features, an embedding dimension of <code>64</code>, <code>4</code> hidden layers of dimension <code>256</code> and ReLU activation for a <code>150k</code> parameter model. Below are snapshots from training that network for 3 minutes per image with a learning rate of <code>1e-4</code>.
		</p>

		<div class="figure_container">
			<a data-fancybox="gallery_baseline" href="media/for_post/training_combined_station__farming__stairs__thing__waves.mp4">
				<video class="figure" src="media/for_post/training_combined_station__farming__stairs__thing__waves.mp4" autoplay muted playsinline loop preload="metadata"></video>
			</a>
		</div>
		<div class="figure_caption">
			<span class="caption_title">Figure: </span> Training snapshots for five different INRs, one for each image, during training with highlighted MSE and PSNR at each frame.
		</div>

		<p>
			In this example the loss is extremely rapidly decreasing in the start and plateaus at rather high level. However the level at which the loss curves plateau differ greatly for the different images. We see for example that the farming image is hard to learn, most likely due to its significant high frequency components.
		</p>

		<div class="figure_container_small">
			<a data-fancybox="gallery_baseline" href="media/for_post/loss_curve_combined.png">
				<img class="figure" src="media/for_post/loss_curve_combined.png"></img>
			</a>
		</div>
		<div class="figure_caption">
			<span class="caption_title">Figure: </span> Loss curve for the stairs image in log scale.
		</div>

		<p>
			We now move to improving this performance by using the average PSNR across these five images as our objective function.
		</p>
		
		<hr class="squiggly-line"/>

		<h2>Scaling up standard MLPs</h2>

		<p>
			Our first step will be to try out a larger MLP since performance seems to saturate rather quickly.
		</p>

		<div class="figure_container">
			<a style="float: left; width: 31%;" data-fancybox="gallery1" href="media/for_post/sweep_hidden_dim_vs_n_layers_mse_combined.png">
				<img class="figure" src="media/for_post/sweep_hidden_dim_vs_n_layers_mse_combined.png"></img>
			</a>
			<a style="float: left; width: 31%;" data-fancybox="gallery1" href="media/for_post/sweep_hidden_dim_vs_n_layers_psnr_combined.png">
				<img class="figure" src="media/for_post/sweep_hidden_dim_vs_n_layers_psnr_combined.png"></img>
			</a>
			<a style="float: left; width: 38%;" data-fancybox="gallery1" href="media/for_post/loss_curve_combined_v2.png">
				<img class="figure" src="media/for_post/loss_curve_combined_v2.png"></img>
			</a>
		</div>
		<div class="figure_caption">
			<span class="caption_title">Figure: </span> MSE and PSNR results for MLPs with different <code>hidden_dim</code> (x-axis) and <code>n_layers</code> (y-axis). The loss curves are with the optimal choice.
		</div>

		<p>
			Based on this information we set <code>n_layers = 6</code> and <code>hidden_dim = 1024</code> but leave a note about trying out larger values for these later. The model now has <code>4.3M</code> parameters but we remark that we have chosen to only use training time as our restriction. Obviously this is not the correct restriction if one is attempting to, e.g., compress images using an INR, in which case we would probably have a fixed parameter budget as well.  
		</p>

		<p>
			We could definitely have pushed MLP size further at this point but remark that when we have no positional encodings, we get stuck in this sort of plateau where we can get performance gains from increasing the parameter count but we are fundamentally unable to express the details of the image with it and are only digging deeper into a local minimum. For this reason we next up look at encoding dimension and frequency scale. Recall that the encoding dimension is the output dimension of $\gamma$ and the frequency scale is the standard deviation of the Gaussian which the frequencies are taken from.
		</p>

		<div class="figure_container_small">
			<a style="float: left; width: 50%;" data-fancybox="gallery2" href="media/for_post/sweep_freq_scale_vs_encoding_dim_mse_combined.png">
				<img class="figure" src="media/for_post/sweep_freq_scale_vs_encoding_dim_mse_combined.png"></img>
			</a>
			<a style="float: left; width: 50%;" data-fancybox="gallery2" href="media/for_post/sweep_freq_scale_vs_encoding_dim_psnr_combined.png">
				<img class="figure" src="media/for_post/sweep_freq_scale_vs_encoding_dim_psnr_combined.png"></img>
			</a>
		</div>
		<div class="figure_caption">
			<span class="caption_title">Figure: </span> MSE and PSNR results for INRs with different <code>encoding_dim</code> (x-axis) and <code>freq_scale</code> (y-axis).
		</div>

		<p>
			Here we get a large jump in average PSNR from <code>26 dB</code> to <code>35 dB</code>. Setting <code>encoding_dim = 256</code> and <code>freq_scale = 80</code> bumps the parameter count slightly to <code>4.5M</code>. We now move to tuning the learning rate. In this case, just looking at the average MSE or PSNR does not tell the whole story since there is a large discrepancy between the images.
		</p>

		<div class="figure_container">
			<a style="float: left; width: 50%;" data-fancybox="gallery3" href="media/for_post/sweep_lr_loss_curves_stairs.png">
				<img class="figure" src="media/for_post/sweep_lr_loss_curves_stairs.png"></img>
			</a>
			<a style="float: left; width: 50%;" data-fancybox="gallery3" href="media/for_post/sweep_lr_loss_curves_thing.png">
				<img class="figure" src="media/for_post/sweep_lr_loss_curves_thing.png"></img>
			</a>
			<a style="float: left; width: 33.33%;" data-fancybox="gallery3" href="media/for_post/sweep_lr_loss_curves_farming.png">
				<img class="figure" src="media/for_post/sweep_lr_loss_curves_farming.png"></img>
			</a>
			<a style="float: left; width: 33.33%;" data-fancybox="gallery3" href="media/for_post/sweep_lr_loss_curves_station.png">
				<img class="figure" src="media/for_post/sweep_lr_loss_curves_station.png"></img>
			</a>
			<a style="float: left; width: 33.33%;" data-fancybox="gallery3" href="media/for_post/sweep_lr_loss_curves_waves.png">
				<img class="figure" src="media/for_post/sweep_lr_loss_curves_waves.png"></img>
			</a>
		</div>
		<div class="figure_caption">
			<span class="caption_title">Figure: </span> Loss curves for the five images with the AdamW optimizer.
		</div>

		<p>
			We see that stairs and thing are particularly unstable at higher learning rates. Meanwhile the arguably optimal learning rate <code>1e-4</code> for those performs poorly for the other three images. An interpretation is that learning rate can greatly benefit from being tuned for the data set (image). Since we are looking for one training recipe that is strong throughout, we instead try the <a target="_blank" href="https://github.com/facebookresearch/schedule_free">ScheduleFree version of AdamW</a>. This is an optimizer which is supposed to be more adaptive than vanilla AdamW which is what we also see in our results.
		</p>

		<div class="figure_container">
			<a style="float: left; width: 50%;" data-fancybox="gallerysf" href="media/for_post/sfsweep_lr_loss_curves_stairs.png">
				<img class="figure" src="media/for_post/sfsweep_lr_loss_curves_stairs.png"></img>
			</a>
			<a style="float: left; width: 50%;" data-fancybox="gallerysf" href="media/for_post/sfsweep_lr_loss_curves_thing.png">
				<img class="figure" src="media/for_post/sfsweep_lr_loss_curves_thing.png"></img>
			</a>
			<a style="float: left; width: 33.33%;" data-fancybox="gallerysf" href="media/for_post/sfsweep_lr_loss_curves_farming.png">
				<img class="figure" src="media/for_post/sfsweep_lr_loss_curves_farming.png"></img>
			</a>
			<a style="float: left; width: 33.33%;" data-fancybox="gallerysf" href="media/for_post/sfsweep_lr_loss_curves_station.png">
				<img class="figure" src="media/for_post/sfsweep_lr_loss_curves_station.png"></img>
			</a>
			<a style="float: left; width: 33.33%;" data-fancybox="gallerysf" href="media/for_post/sfsweep_lr_loss_curves_waves.png">
				<img class="figure" src="media/for_post/sfsweep_lr_loss_curves_waves.png"></img>
			</a>
		</div>
		<div class="figure_caption">
			<span class="caption_title">Figure: </span> Loss curves for the five images with the ScheduleFree AdamW optimizer.
		</div>

		<p>
			It is now easier to compare the five learning rates and we find that <code>5e-4</code> is optimal in this case.
		</p>

		<div class="figure_container">
			<a style="float: left; width: 50%;" data-fancybox="gallerysfc" href="media/for_post/sfsweep_lr_mse_combined.png">
				<img class="figure" src="media/for_post/sfsweep_lr_mse_combined.png"></img>
			</a>
			<a style="float: left; width: 50%;" data-fancybox="gallerysfc" href="media/for_post/sfsweep_lr_psnr_combined.png">
				<img class="figure" src="media/for_post/sfsweep_lr_psnr_combined.png"></img>
			</a>
		</div>
		<div class="figure_caption">
			<span class="caption_title">Figure: </span> Average MSE and PSNR for the learning rates from <code>1e-4</code> to <code>5e-4</code>.
		</div>

		<p>
			Still, we choose to set <code>lr = 4e-4</code> just for the additional stability since we see that higher learning rates lead to oscillations in the loss once it is low enough. 
		</p>

		<p>
			At this point, the PSNR is high enough that the images are almost perfectly captured by the INR. Below is a version of the training video from earlier with this model.
		</p>

		<div class="figure_container">
			<a data-fancybox="galleryvidv2" href="media/for_post/training_combined_station__farming__stairs__thing__waves_v2.mp4">
				<video class="figure" src="media/for_post/training_combined_station__farming__stairs__thing__waves_v2.mp4" autoplay muted playsinline loop preload="metadata"></video>
			</a>
		</div>
		<div class="figure_caption">
			<span class="caption_title">Figure: </span> Training snapshots for five different INRs, one for each image, during training with highlighted MSE and PSNR at each frame. Put video in fullscreen to see details better.
		</div>

		<p>
			Next up we look at learnable frequencies. This means that we still initialize the frequencies in the positional encodings with standard deviation <code>80</code> but treat them as learnable model parameters. Here we see that trainable encodings do not improve performance.
		</p>

		<div class="figure_container">
			<a style="float: left; width: 50%;" data-fancybox="gallery4" href="media/for_post/train_enc_sweep_trainable_encodings_mse_combined.png">
				<img class="figure" src="media/for_post/train_enc_sweep_trainable_encodings_mse_combined.png"></img>
			</a>
			<a style="float: left; width: 50%;" data-fancybox="gallery4" href="media/for_post/train_enc_sweep_trainable_encodings_psnr_combined.png">
				<img class="figure" src="media/for_post/train_enc_sweep_trainable_encodings_psnr_combined.png"></img>
			</a>
		</div>
		<div class="figure_caption">
			<span class="caption_title">Figure: </span> MSE and PSNR for trainable and fixed frequencies.
		</div>

		<p>
			The final choice we look at is the activation function. Since we are using positional encodings, we are only comparing some standard choices, specifically ReLU, GELU and Swish. So far all our sweeping has been done with ReLU so it perhaps should not be a surprise that it performs best. Still, its margin is at least slightly remarkable.
		</p>

		<div class="figure_container">
			<a style="float: left; width: 50%;" data-fancybox="gallery5" href="media/for_post/lt_sweep_layer_type_mse_combined.png">
				<img class="figure" src="media/for_post/lt_sweep_layer_type_mse_combined.png"></img>
			</a>
			<a style="float: left; width: 50%;" data-fancybox="gallery5" href="media/for_post/lt_sweep_layer_type_psnr_combined.png">
				<img class="figure" src="media/for_post/lt_sweep_layer_type_psnr_combined.png"></img>
			</a>
		</div>
		<div class="figure_caption">
			<span class="caption_title">Figure: </span> MSE and PSNR comparison between different activation functions.
		</div>

		<p>
			The last property we look at is Gabor feature encodings. Just as for the activation function, we have so far made the (few) decisions about the architecture to optimize for Fourier features which puts Gabor features at a disadvantage. Since for Gabor features each dimension in <code>encoding_dim</code> has less energy since it is localized in position, we explore higher dimensional encoding dimensions at the same time.
		</p>

		<div class="figure_container">
			<a style="float: left; width: 50%;" data-fancybox="gallery6" href="media/for_post/pe_sweep_position_encoding_vs_encoding_dim_mse_combined.png">
				<img class="figure" src="media/for_post/pe_sweep_position_encoding_vs_encoding_dim_mse_combined.png"></img>
			</a>
			<a style="float: left; width: 50%;" data-fancybox="gallery6" href="media/for_post/pe_sweep_position_encoding_vs_encoding_dim_psnr_combined.png">
				<img class="figure" src="media/for_post/pe_sweep_position_encoding_vs_encoding_dim_psnr_combined.png"></img>
			</a>
			<a style="float: left; width: 20%;" data-fancybox="gallery6" href="media/for_post/pe_sweep_position_encoding_vs_encoding_dim_psnr_farming.png">
				<img class="figure" src="media/for_post/pe_sweep_position_encoding_vs_encoding_dim_psnr_farming.png"></img>
			</a>
			<a style="float: left; width: 20%;" data-fancybox="gallery6" href="media/for_post/pe_sweep_position_encoding_vs_encoding_dim_psnr_stairs.png">
				<img class="figure" src="media/for_post/pe_sweep_position_encoding_vs_encoding_dim_psnr_stairs.png"></img>
			</a>
			<a style="float: left; width: 20%;" data-fancybox="gallery6" href="media/for_post/pe_sweep_position_encoding_vs_encoding_dim_psnr_station.png">
				<img class="figure" src="media/for_post/pe_sweep_position_encoding_vs_encoding_dim_psnr_station.png"></img>
			</a>
			<a style="float: left; width: 20%;" data-fancybox="gallery6" href="media/for_post/pe_sweep_position_encoding_vs_encoding_dim_psnr_thing.png">
				<img class="figure" src="media/for_post/pe_sweep_position_encoding_vs_encoding_dim_psnr_thing.png"></img>
			</a>
			<a style="float: left; width: 20%;" data-fancybox="gallery6" href="media/for_post/pe_sweep_position_encoding_vs_encoding_dim_psnr_waves.png">
				<img class="figure" src="media/for_post/pe_sweep_position_encoding_vs_encoding_dim_psnr_waves.png"></img>
			</a>
		</div>
		<div class="figure_caption">
			<span class="caption_title">Figure: </span> Average MSE and PSNR when sweeping Fourier vs Gabor features at the same time as encoding dimension. In the bottom row we have the per-image PSNRs.
		</div>

		<p>
			The result is very even with Gabor feature coming out on top for PSNR but not MSE but only at higher encoding dimension. Note meanwhile that the MSE and PSNR for Fourier features barely improve when going from <code>192</code> to <code>384</code> encoding dimensions. A conclusion that may be made is that Fourier features benefit less from larger encoding dimension but is preferable for a smaller model.
		</p>

		<p>
			Gabor features have the additional effective hyperparameter deciding how to choose the widths of the Gabor atoms. In the code, we have set this to $0.1 \sqrt{\frac{d_{enc}}{256}}$ where $d_{enc}$ is the per-dimension encoding dimension so half of our actual encoding dimension in the 2D case. This choice was made to avoid an additional hyperparameter but would obviously benefit from tuning. Still, we stick with Gabor features for now.
		</p>

		<hr class="squiggly-line"/>

		<h2>Improving MLPs</h2>

		<p>
			We now turn to the three techniques of improving MLPs mentioned earlier, mixture-of-experts, LayerNorm and skip connections. LayerNorm generally has the effect of stabilizing gradient flow which is more important for deeper networks. With how we have pushed the model so far, it is thus conceivable that it should possibly help in this case. We find that it does so greatly.
		</p>

		<div class="figure_container">
			<a style="float: left; width: 50%;" data-fancybox="gallery7" href="media/for_post/sweep_layernorm_mse_combined.png">
				<img class="figure" src="media/for_post/sweep_layernorm_mse_combined.png"></img>
			</a>
			<a style="float: left; width: 50%;" data-fancybox="gallery7" href="media/for_post/sweep_layernorm_psnr_combined.png">
				<img class="figure" src="media/for_post/sweep_layernorm_psnr_combined.png"></img>
			</a>
			<a style="float: left; width: 20%;" data-fancybox="gallery7" href="media/for_post/sweep_layernorm_loss_curves_farming.png">
				<img class="figure" src="media/for_post/sweep_layernorm_loss_curves_farming.png"></img>
			</a>
			<a style="float: left; width: 20%;" data-fancybox="gallery7" href="media/for_post/sweep_layernorm_loss_curves_stairs.png">
				<img class="figure" src="media/for_post/sweep_layernorm_loss_curves_stairs.png"></img>
			</a>
			<a style="float: left; width: 20%;" data-fancybox="gallery7" href="media/for_post/sweep_layernorm_loss_curves_station.png">
				<img class="figure" src="media/for_post/sweep_layernorm_loss_curves_station.png"></img>
			</a>
			<a style="float: left; width: 20%;" data-fancybox="gallery7" href="media/for_post/sweep_layernorm_loss_curves_thing.png">
				<img class="figure" src="media/for_post/sweep_layernorm_loss_curves_thing.png"></img>
			</a>
			<a style="float: left; width: 20%;" data-fancybox="gallery7" href="media/for_post/sweep_layernorm_loss_curves_waves.png">
				<img class="figure" src="media/for_post/sweep_layernorm_loss_curves_waves.png"></img>
			</a>
			<a style="float: left; width: 20%;" data-fancybox="gallery7" href="media/for_post/sweep_layernorm_psnr_curves_farming.png">
				<img class="figure" src="media/for_post/sweep_layernorm_psnr_curves_farming.png"></img>
			</a>
			<a style="float: left; width: 20%;" data-fancybox="gallery7" href="media/for_post/sweep_layernorm_psnr_curves_stairs.png">
				<img class="figure" src="media/for_post/sweep_layernorm_psnr_curves_stairs.png"></img>
			</a>
			<a style="float: left; width: 20%;" data-fancybox="gallery7" href="media/for_post/sweep_layernorm_psnr_curves_station.png">
				<img class="figure" src="media/for_post/sweep_layernorm_psnr_curves_station.png"></img>
			</a>
			<a style="float: left; width: 20%;" data-fancybox="gallery7" href="media/for_post/sweep_layernorm_psnr_curves_thing.png">
				<img class="figure" src="media/for_post/sweep_layernorm_psnr_curves_thing.png"></img>
			</a>
			<a style="float: left; width: 20%;" data-fancybox="gallery7" href="media/for_post/sweep_layernorm_psnr_curves_waves.png">
				<img class="figure" src="media/for_post/sweep_layernorm_psnr_curves_waves.png"></img>
			</a>
		</div>
		<div class="figure_caption">
			<span class="caption_title">Figure: </span> MSE and PSNR average results and training curves for LayerNorm on and off.
		</div>

		<p>
			While this looks like a clear win for LayerNorm, if we look closer at the MSE for stairs (image #2) and waves (image #5), we see that MSE dips slightly lower at its best. However for PSNR the win for <code>layernorm = True</code> is clear and so we adopt it. Note also in the figures how much faster the network is when training with LayerNorm active. During the preparation of this post I also looked at RMSNorm and pre/post LayerNorm but found no big differences. This implementation is for post-LayerNorm.
		</p>

		<p>
			Since LayerNorm has effectively given us a boost in performance, we turn to MoE to check if it is better spent on another copy of the network, more parameters, or keep spending the extra time training deeper. On a conceptual layer, MoEs for INRs are appealing since you could have different experts learn different parts of the image.
		</p>

		<div class="figure_container">
			<a style="float: left; width: 33.3333%;" data-fancybox="gallery8" href="media/for_post/sweep_n_experts_vs_hidden_dim_vs_n_layers_psnr_combined.png">
				<img class="figure" src="media/for_post/sweep_n_experts_vs_hidden_dim_vs_n_layers_psnr_combined.png"></img>
			</a>
			<a style="float: left; width: 33.3333%;" data-fancybox="gallery8" href="media/for_post/sweep_n_experts_vs_hidden_dim_vs_n_layers_psnr_farming.png">
				<img class="figure" src="media/for_post/sweep_n_experts_vs_hidden_dim_vs_n_layers_psnr_farming.png"></img>
			</a>
			<a style="float: left; width: 33.3333%;" data-fancybox="gallery8" href="media/for_post/sweep_n_experts_vs_hidden_dim_vs_n_layers_psnr_stairs.png">
				<img class="figure" src="media/for_post/sweep_n_experts_vs_hidden_dim_vs_n_layers_psnr_stairs.png"></img>
			</a>
			<a style="float: left; width: 33.3333%;" data-fancybox="gallery8" href="media/for_post/sweep_n_experts_vs_hidden_dim_vs_n_layers_psnr_station.png">
				<img class="figure" src="media/for_post/sweep_n_experts_vs_hidden_dim_vs_n_layers_psnr_station.png"></img>
			</a>
			<a style="float: left; width: 33.3333%;" data-fancybox="gallery8" href="media/for_post/sweep_n_experts_vs_hidden_dim_vs_n_layers_psnr_thing.png">
				<img class="figure" src="media/for_post/sweep_n_experts_vs_hidden_dim_vs_n_layers_psnr_thing.png"></img>
			</a>
			<a style="float: left; width: 33.3333%;" data-fancybox="gallery8" href="media/for_post/sweep_n_experts_vs_hidden_dim_vs_n_layers_psnr_waves.png">
				<img class="figure" src="media/for_post/sweep_n_experts_vs_hidden_dim_vs_n_layers_psnr_waves.png"></img>
			</a>
		</div>
		<div class="figure_caption">
			<span class="caption_title">Figure: </span> PSNR for different number of layer and hidden dimensions, compared between 1 and 2 experts. Average is in the top left image.
		</div>

		<p>
			For the results we find a very slight edge for <code>n_experts = 2</code> but at two training runs per configuration and image, the result is not statistically significant. Still, we keep it and lower <code>n_layers</code> to 5 to get a model with <code>6.8M</code> parameters with a PSNR of <code>47.15</code>.
		</p>

		<p>
			Lastly skip connections turns out to have a small negative effect on performance, as well as slow down training slightly due to the increased computational load. It is possible skip connections do not become important until the networks are much deeper.
		</p>

		<p>
			At this point, with a PSNR at <code>47.15</code>, the images are essentially perfectly captured by the networks. To some degree, this is the point of this post - by properly tweaking the architecture we can get a standard Fourier features MLP to have perfect performance with only three minutes of training on a modern GPU. Looking at the snapshots, even after only 5 epochs at a batch size of <code>4096 = 768×768 × 0.7%</code>, the image is mostly learnt.
		</p>

		<div class="figure_container">
			<a data-fancybox="galleryvidv3" href="media/for_post/training_combined_station__farming__stairs__thing__waves_v3.mp4">
				<video class="figure" src="media/for_post/training_combined_station__farming__stairs__thing__waves_v3.mp4" autoplay muted playsinline loop preload="metadata"></video>
			</a>
		</div>
		<div class="figure_caption">
			<span class="caption_title">Figure: </span> Training snapshots for five different INRs, one for each image, during training with highlighted MSE and PSNR at each frame. Average final PSNR is <code>47.15</code>. Put video in fullscreen to see details better.
		</div>

		<p>
			On the topic of <code>batch_size</code>, our choice of <code>4096</code> is indeed optimal among the canonical $2^n$ alternatives.
		</p>

		<div class="figure_container">
			<a style="float: left; width: 50%;" data-fancybox="gallery7" href="media/for_post/sweep_batch_size_mse_combined.png">
				<img class="figure" src="media/for_post/sweep_batch_size_mse_combined.png"></img>
			</a>
			<a style="float: left; width: 50%;" data-fancybox="gallery7" href="media/for_post/sweep_batch_size_psnr_combined.png">
				<img class="figure" src="media/for_post/sweep_batch_size_psnr_combined.png"></img>
			</a>
			<a style="float: left; width: 20%;" data-fancybox="gallery7" href="media/for_post/sweep_batch_size_loss_curves_farming.png">
				<img class="figure" src="media/for_post/sweep_batch_size_loss_curves_farming.png"></img>
			</a>
			<a style="float: left; width: 20%;" data-fancybox="gallery7" href="media/for_post/sweep_batch_size_loss_curves_stairs.png">
				<img class="figure" src="media/for_post/sweep_batch_size_loss_curves_stairs.png"></img>
			</a>
			<a style="float: left; width: 20%;" data-fancybox="gallery7" href="media/for_post/sweep_batch_size_loss_curves_station.png">
				<img class="figure" src="media/for_post/sweep_batch_size_loss_curves_station.png"></img>
			</a>
			<a style="float: left; width: 20%;" data-fancybox="gallery7" href="media/for_post/sweep_batch_size_loss_curves_thing.png">
				<img class="figure" src="media/for_post/sweep_batch_size_loss_curves_thing.png"></img>
			</a>
			<a style="float: left; width: 20%;" data-fancybox="gallery7" href="media/for_post/sweep_batch_size_loss_curves_waves.png">
				<img class="figure" src="media/for_post/sweep_batch_size_loss_curves_waves.png"></img>
			</a>
			<a style="float: left; width: 20%;" data-fancybox="gallery7" href="media/for_post/sweep_batch_size_psnr_curves_farming.png">
				<img class="figure" src="media/for_post/sweep_batch_size_psnr_curves_farming.png"></img>
			</a>
			<a style="float: left; width: 20%;" data-fancybox="gallery7" href="media/for_post/sweep_batch_size_psnr_curves_stairs.png">
				<img class="figure" src="media/for_post/sweep_batch_size_psnr_curves_stairs.png"></img>
			</a>
			<a style="float: left; width: 20%;" data-fancybox="gallery7" href="media/for_post/sweep_batch_size_psnr_curves_station.png">
				<img class="figure" src="media/for_post/sweep_batch_size_psnr_curves_station.png"></img>
			</a>
			<a style="float: left; width: 20%;" data-fancybox="gallery7" href="media/for_post/sweep_batch_size_psnr_curves_thing.png">
				<img class="figure" src="media/for_post/sweep_batch_size_psnr_curves_thing.png"></img>
			</a>
			<a style="float: left; width: 20%;" data-fancybox="gallery7" href="media/for_post/sweep_batch_size_psnr_curves_waves.png">
				<img class="figure" src="media/for_post/sweep_batch_size_psnr_curves_waves.png"></img>
			</a>
		</div>
		<div class="figure_caption">
			<span class="caption_title">Figure: </span> MSE and PSNR average results and training curves for LayerNorm on and off.
		</div>

		<hr class="squiggly-line"/>

		<h2>Concluding remarks</h2>

		<p>
			In the process of tweaking all the available hyperparameters, we have essentially saturated our benchmark problem. There are two main implications of this worth mentioning:
		</p>
			<ul>
				<li>
					<b>The benchmark problem might have been too easy</b> If we really want to push architectures to their limits, we should place a lower time limit or possibly added a memory limit.
				</li>
				<li>
					<b>When comparing INR architectures, tweaking all variables are very important</b> To some degree, this is an impossible ask. Benchmarks generally fix a lot of parameters and only change one thing. The point of this post is to show that even a simple architecture can be pushed very far with a limited compute budget.
				</li>
			</ul>
		<p>
			Generally, the goal of INRs is not to simply learn a function but to be used for some downstream task involving reconstruction from partial measurements such as superresolution, CT reconstruction or novel view synthesis. These are more appropriate benchmarks but at the same time less general.
		</p>

		<p>
			The usage of LayerNorm in the MLP is perhaps the most important step we took in the optimization procedure. The effectiveness of LayerNorm has been noticed elsewhere (e.g., <a href="https://openaccess.thecvf.com/content/WACV2024/html/Kazerouni_INCODE_Implicit_Neural_Conditioning_With_Prior_Knowledge_Embeddings_WACV_2024_paper.html">INCODE</a>, <a href="https://arxiv.org/pdf/2409.10836v4">SL²A-INR</a>, among others) but is still important to push.
		</p>


	</section>
	
	<link rel="stylesheet" href="../fancybox.css" />
	<script src="../fancybox.umd.js"></script>

	<script>
		Fancybox.bind("[data-fancybox]", {
			closeButton: false,
		});
	  </script>

</body>
</html>
