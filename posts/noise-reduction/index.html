<!DOCTYPE html>
<html lang="en">
<head>
	<title>On the role of phase in ML audio noise reduction</title>

	<meta name="viewport" content="width=device-width, initial-scale=1">
	<meta name="theme-color" content="#FBEDEA" />
	<meta charset="utf-8">

	<link rel="shortcut icon" type="image/x-icon"  href="../../favicon.ico?">
	<link rel="apple-touch-icon" href="../../apple-touch-icon.png">

	<link rel="stylesheet" href="style.css">

	<script>
	window.MathJax = {
	tex: {
		inlineMath: [['$', '$'], ['\\(', '\\)']]
	}
	};
	</script>

	<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

	<link rel="preconnect" href="https://fonts.googleapis.com">
	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>

	<link href="https://fonts.googleapis.com/css2?family=Roboto+Mono:wght@400;500;600;700&display=swap" rel="stylesheet">

	<link rel="stylesheet" href="../atom-one-dark.min.css">
	<script src="../highlight.min.js"></script>

	<script>hljs.highlightAll();</script>

</head>
<body>
	<div class="blog centering" id="back_container">
		<a href="../index.html" class="back"><svg xmlns="http://www.w3.org/2000/svg" width="18" height="14" fill="currentColor" class="bi bi-caret-left" viewBox="0 0 22 18">
			<path fill-rule="evenodd" clip-rule="evenodd" d="M14.0303 7.46967C14.3232 7.76256 14.3232 8.23744 14.0303 8.53033L10.5607 12L14.0303 15.4697C14.3232 15.7626 14.3232 16.2374 14.0303 16.5303C13.7374 16.8232 13.2626 16.8232 12.9697 16.5303L8.96967 12.5303C8.67678 12.2374 8.67678 11.7626 8.96967 11.4697L12.9697 7.46967C13.2626 7.17678 13.7374 7.17678 14.0303 7.46967Z"/>
		  </svg>All posts</a>
	</div>

	<section class="blog centering post">
		<h1>On the role of phase in ML audio noise reduction</h1>
		<div class="subtitle monospace">By Simon Halvdansson&nbsp;|&nbsp;Feb. 2026</div>

		<hr class="squiggly-line"/>

		<p>
			A standard way of reducing the noise of audio or another signal with some form of time-frequency structure using a machine learning approach is as follows: Compute the short-time Fourier transform of the signal, map it to a "clean" spectrogram using a U-Net, and then synthesize a signal back. In the simplest version of this construction, the U-Net returns a real-valued mask over the time-frequency plane, telling us which time-frequency bins to ignore in the reconstruction and which to keep. This approach reuses the noisy phase value of the original signal.
		</p>

		<p>
			In this post we look at more intricate versions of this construction that go beyond time-frequency multipliers by paying more attention to the phase of the output.
		</p>

		
		<div class="figure_container_small">
			<a data-fancybox="gallery0" href="media/complex_output/denoise_compare.png">
				<img class="figure" src="media/complex_output/denoise_compare.png"></img>
			</a>
		</div>
		<div class="figure_caption">
			<span class="caption_title">Figure:</span> Denoising example with an STFT -> STFT U-Net.
		</div>

		<p>
			After covering some background and context, we will discuss three STFT U-Net based noise reduction methods which all utilize phase in different ways. The goal is not to determine which one is optimal, that is dependent on the specific task, but rather to show that all three work and discuss their respective strengths.
		</p>

		<hr class="squiggly-line">

		<h2>Background</h2>

		<p>
			As to keep the post focused, we will assume the reader has a basic familiarity with the short-time Fourier transform (STFT) and spectrograms as 2D representations of signals which allows simultaneous inspection of the time and frequency contents. A key fact about the STFT is that any signal (vector) has a STFT (2D matrix) but not all 2D matrices (of the correct shape) correspond to a signal. This can be seen as a consequence of the <a target="_blank" href="https://en.wikipedia.org/wiki/Uncertainty_principle">uncertainty principle</a> which puts limits on the time-frequency localization of a signal. 
		</p>

		<h3 id="phase_background">Phase in the STFT</h3>
		<p>
			Since the STFT is complex-valued, it is often visualized via its absolute values, the spectrogram. The role of the phase of the STFT was not clear in the <a href="https://www.sciencedirect.com/science/article/abs/pii/S0167639316300784" target="_blank">early days of time frequency analysis</a>. Indeed, energy is a much more familiar concept and that is determine only by the absolute value of the STFT. Moreover, as <a href="https://arxiv.org/pdf/2006.10579">phase retrieval</a> has shown, knowing spectrogram the phase can be recovered up to a global unimodular factor.
		</p>

		<p>
			So what happens if we have a 2D matrix of the correct shape of complex numbers and want to synthesize a signal from it but the phase and/or magnitude are not chosen so that they correspond to an actual signal? Such a matrix is said to be <i>inconsistent</i> but if we still do synthesize a signal by means of the inverse STFT, we do get a signal. What happens is that we implicitly compute the orithogonal projection onto the subspace of consistent matrices, the <a>Gabor projection</a>. This is fine, after all orthogonal projections are the best kind, but the resulting audio signal may sound a little weird to human ears.
		</p>

		<p>
			The best way to illustrate this is by an example. A pure sine tone has a sharp frequency concentration and a predictable STFT phase behavior, the phase changes evenly over time. 
		</p>

		<div class="figure_container">
			<a data-fancybox="gallery0" href="media/sine.png">
				<img class="figure" src="media/sine.png"></img>
			</a>
		</div>
		<div class="figure_caption">
			<span class="caption_title">Figure:</span> STFT of pure sine tone at 440 Hz visualized as magnitude, phase, and phase gradients.
		</div>
		
		<p>
			If we take this STFT and randomize the phase and synthesize a signal back, we get a much less nicely structured STFT from the resulting waveform.
		</p>

		<div class="figure_container">
			<a data-fancybox="gallery0" href="media/modified_sine.png">
				<img class="figure" src="media/modified_sine.png"></img>
			</a>
		</div>
		<div class="figure_caption">
			<span class="caption_title">Figure:</span> STFT of modified pure sine tone at 440 Hz visualized as magnitude, phase, and phase gradients.
		</div>

		<p>
			Here we see that the Gabor projection has changed the magnitude of the STFT as well, even though it is only tha phases we changed. The phase gradients are a lot less even which is the main takeaway. The change can be heard in the following comparison.
		</p>

		<div class="figure_container_small">
			<audio style="width: 49%;" controls>
				<source src="media/pure_sine.wav" type="audio/wav">
				Your browser does not support the audio element.
			</audio>
			<audio style="width: 50%;" controls>
				<source src="media/modified_sine.wav" type="audio/wav">
				Your browser does not support the audio element.
			</audio>
		</div>
		<div class="figure_caption">
			<span class="caption_title">Figure:</span> Pure (left) and modified (right) 440 Hz sine tones.
		</div>

		<p>
			Listening to these examples, we hear the same tone but with a weird form of non-pureness in the modified version. It is this type of phase-induced error which we are looking to avoid in this post.
		</p>


		<h3>U-Nets</h3>
		<p>
			We are looking to <i>modify</i> the STFT of our input signal. Forgetting about modern diffusion-based image-to-image models for a while, the standard way to do this is via a <a href="https://en.wikipedia.org/wiki/U-Net" target="_blank">U-Net</a>. 
		</p>

		<div class="figure_container_small">
			<a data-fancybox="gallery0" href="media/unet.png">
				<img class="figure" src="media/unet.png"></img>
			</a>
		</div>
		<div class="figure_caption">
			<span class="caption_title">Figure:</span> Illustration of the general structure of U-Net, from  <a href="https://github.com/ted-17/unet" target="_blank">ted-17 / unet</a>.
		</div>

		<p>
			This architecture is more than 10 years old and should be considered a commodity by now. We reproduce the version used here below for reference.
		</p>

		<pre><code class="language-py">class ConvBlock(torch.nn.Module):
    def __init__(self, in_channels, out_channels):
        super().__init__()
        self.net = torch.nn.Sequential(
            torch.nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),
            torch.nn.ReLU(inplace=True),
            torch.nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),
            torch.nn.ReLU(inplace=True),
        )

    def forward(self, x):
        return self.net(x)


class UNet2D(torch.nn.Module):
    def __init__(self, in_channels=1, base_channels=16, out_channels=1, use_sigmoid=True):
        super().__init__()
        self.enc1 = ConvBlock(in_channels, base_channels)
        self.enc2 = ConvBlock(base_channels, base_channels * 2)
        self.enc3 = ConvBlock(base_channels * 2, base_channels * 4)
        self.enc4 = ConvBlock(base_channels * 4, base_channels * 8)
        self.bottleneck = ConvBlock(base_channels * 8, base_channels * 16)

        self.pool = torch.nn.MaxPool2d(2)

        self.dec4 = ConvBlock(base_channels * 16 + base_channels * 8, base_channels * 8)
        self.dec3 = ConvBlock(base_channels * 8 + base_channels * 4, base_channels * 4)
        self.dec2 = ConvBlock(base_channels * 4 + base_channels * 2, base_channels * 2)
        self.dec1 = ConvBlock(base_channels * 2 + base_channels, base_channels)

        self.out_conv = torch.nn.Conv2d(base_channels, out_channels, kernel_size=1)
        self.use_sigmoid = use_sigmoid

    def _upsample_to(self, x, target):
        return F.interpolate(x, size=target.shape[-2:], mode="nearest")

    def forward(self, x):
        e1 = self.enc1(x)
        e2 = self.enc2(self.pool(e1))
        e3 = self.enc3(self.pool(e2))
        e4 = self.enc4(self.pool(e3))
        b = self.bottleneck(self.pool(e4))

        d4 = self._upsample_to(b, e4)
        d4 = self.dec4(torch.cat([d4, e4], dim=1))

        d3 = self._upsample_to(d4, e3)
        d3 = self.dec3(torch.cat([d3, e3], dim=1))

        d2 = self._upsample_to(d3, e2)
        d2 = self.dec2(torch.cat([d2, e2], dim=1))

        d1 = self._upsample_to(d2, e1)
        d1 = self.dec1(torch.cat([d1, e1], dim=1))

        output = self.out_conv(d1)
        if self.use_sigmoid:
            output = torch.sigmoid(output)
        return output
</code></pre>

		<hr class="squiggly-line">

		<h2>Dataset</h2>
		
		<p>
			Since our focus is more on the method than the data and performance, we take a simple approach to the data. Specifically, we use the <a target="_blank" href="https://datacollective.mozillafoundation.org/datasets/cmj8u3pud00q9nxxbcmq6uz24">Common Voice Scripted Speech 24.0 - Swedish</a> subset which is <code>1.13 GB</code>. This dataset is not perfectly noise-free nor is it incredibly large but it is easily accessible and generally of high quality. To create noisy data, we add white noise to the waveform with energy between 0% and 60% of that of the waveform, then normalize the noisy version to have energy 1 and use the same scale factor for the clean version to that the target has less energy but the network does not know a priori how much energy it is supposed to have.
		</p>

		$$
		f_{\text{noisy}} = \frac{f + s\frac{\mathcal{N}}{\Vert \mathcal{N} \Vert}}{\big\Vert f + s\frac{\mathcal{N}}{\Vert \mathcal{N} \Vert} \big\Vert},\qquad f_{\text{clean}} = \frac{f}{\big\Vert f + s\frac{\mathcal{N}}{\Vert \mathcal{N} \Vert} \big\Vert},\qquad s \sim U(0, 0.6).
		$$

		<p>
			We could also have added different sorts of noise $\mathcal{N}$ (pink noise, environmental noise, etc.) to make the algorithm more robust but we skip this in the interest of simplicity.
		</p>

		<hr class="squiggly-line">

		<h2>Take 1: Time-frequency multiplier</h2>

		<p>
			If we want to attack noise reduction using machine learning using we need to identify the loss function we wish to train using. While mean squared error (MSE) is a tried and true method, it has a clear disadvantage when it comes to audio tasks: The human ear does not hear (waveform) phase. For example, we cannot distinguish between $f$ and $-f$, yet $\operatorname{MSE}(f, -f) > 0$. There is a classical <a href="https://www.soundsandwords.io/audio-loss-functions/" target="_blank">blog post</a> by Evan Radkoff on loss functions in Audio ML with many details and suggestions but we will settle with just looking at the spectrogram $L^1$ distance, defined as $\Vert \operatorname{SPEC}(f) - \operatorname{SPEC}(g) \Vert_{L^1}$. This loss function is phase invariant and easy to understand; we want to match the time-frequency distribution of the energy. 
		</p>

		<p>
			Now in this simplest version of noise reduction, the U-Net will only act to modify the magntudes of the STFT and only take the magnitudes as input data to the U-Net. This means that, with $V_g f$ the STFT and $V_g f^*$ the inverse STFT, the pipeline can be written as
		</p>
			<div class="equation">
				$
				V_g^* \big(\operatorname{U-Net}( |V_gf|) \cdot V_g f\big) = A_g^{\operatorname{U-Net}( |V_gf| )} f
				$
			</div>
		<p>
			where $A_g^m$ is a so-called <i>Gabor multiplier</i> with mask $m$. While Gabor multipliers are nicely behaved linear maps, U-Nets very much are not, which is the whole point. 
		</p>

		<p>
			Programmatically, we can write this model as follows:
		</p>

<pre><code class="language-py">class SpectrogramMaskUNet(torch.nn.Module):
    def __init__(self):
        super().__init__()
        self.n_fft = N_FFT
        self.unet = UNet2D(in_channels=1, base_channels=16)
        self.register_buffer("window", torch.hann_window(WINDOW_LENGTH))

    def forward(self, waveform):
        stft = torch.stft(
            waveform.squeeze(1),
            n_fft=N_FFT,
            hop_length=HOP_LENGTH,
            win_length=WINDOW_LENGTH,
            window=self.window,
            return_complex=True,
        )
        mag = stft.abs()
        mag_in = mag.unsqueeze(1)
        mask = self.unet(mag_in)
        masked_stft = stft * mask.squeeze(1)

        denoised = torch.istft(
            masked_stft,
            n_fft=N_FFT,
            hop_length=HOP_LENGTH,
            win_length=WINDOW_LENGTH,
            window=self.window,
            length=waveform.shape[-1],
        )

        return denoised.unsqueeze(1), mask
</code></pre>

		<p>
			Training for a few epochs, we get acceptable performance such as in the following example.
		</p>

		<div class="figure_container">
			<a data-fancybox="gallery0" href="media/mask_output/denoise_compare.png">
				<img class="figure" src="media/mask_output/denoise_compare.png"></img>
			</a>
		</div>
		<div class="figure_caption">
			<span class="caption_title">Figure:</span> Spectrograms and the mask of a denoising process. The noisy spectrogram is the input of the U-Net and the mask is the output.
		</div>

		<p>
			Here the mask itself almost looks like a spectrogram because it has identified the specific parts where the actual audio is coming in. The clean spectrogram is also not entirely noise free in this example which most likely is limiting the learning potential of the network. Still, the general principle of there being many ways to be wrong but only one to be right applies here and the denoised version appears to have gotten rid of at least some of the higher frequency artifacts of the clean version.
		</p>

		<p>
			The audio for this particular example can be played here.
		</p>

		<div class="figure_container">
			<audio style="width: 32%;" controls>
				<source src="media/mask_output/clean.wav" type="audio/wav">
				Your browser does not support the audio element.
			</audio>
			<audio style="width: 33%;" controls>
				<source src="media/mask_output/noisy.wav" type="audio/wav">
				Your browser does not support the audio element.
			</audio>
			<audio style="width: 33%;" controls>
				<source src="media/mask_output/denoised.wav" type="audio/wav">
				Your browser does not support the audio element.
			</audio>
		</div>
		<div class="figure_caption">
			<span class="caption_title">Figure:</span> Clean (left), noisy (middle) denoised (right).
		</div>

		<p>
			Below is the loss curve for this training run.
		</p>

		<div class="figure_container_small">
			<a data-fancybox="gallery0" href="media/mask_output/loss_curve.png">
				<img class="figure" src="media/mask_output/loss_curve.png"></img>
			</a>
		</div>
		<div class="figure_caption">
			<span class="caption_title">Figure:</span> Loss curve (smoothened) during training of masked spectrogram U-Net.
		</div>

		<p>
			Here we see a relatively quick plateau. Most likely this is due to the task being simple due to there only being white noise and the presence of noise in the target waveforms as well.
		</p>

		<p>
			Note that since we have not changed the phase of the noisy spectrogram, we subject to the noisy phase messing up the resulting signal in the same way as we illustrated <a href="#phase_background">earlier</a>. This is a clear limitation but in return we get a method which is very structured. By forcing the output to use the phase data from the original noisy output and capping the mask at $1$ we place limits on how much we can diverge from the original (noisy) input signal. This can be valuable due to the high explainability of this method. In this next method, we trade some of this explainability for expressiveness.
		</p>

		<hr class="squiggly-line">

		<h2>Take 2: 2-channel complex STFT prediction</h2>

		<p>
			One way to lift the limitations just discussed with the masking approach is to give up the multiplicative formulation. Specifically, we can let a U-Net map the noisy STFT to a 2D matrix of complex numbers of the same shape and then synthesize a signal from that. By doing so, we move away from the time-frequency structured approach of the earlier method and take a step towards an end-to-end solution.
		</p>

		<p>
			By doing this, we are letting the phases into the U-Net directly. There are a few different ways to go about this. Perhaps the simplest one is to let the U-Net map to and from an image with 2 channels (real + imaginary, or magnitude + phase). Predicting phase in $[-\pi, \pi]$ is difficult due to the wrapping behavior so we go with the continuous approach of predicting real and imaginary values as two channels.
		</p>

		<p>
			The only modifications needed on the model side is to <i>not</i> apply sigmoid activation to the last layer of the U-Net and to encode/decode complex numbers in two channels.
		</p>

<pre><code class="language-py">class Spectrogram2ChannelUNet(torch.nn.Module):
    def __init__(self):
        super().__init__()
        self.n_fft = N_FFT
        self.unet = UNet2D(
            in_channels=2,
            base_channels=16,
            out_channels=2,
            use_sigmoid=False,
        )
        self.register_buffer("window", torch.hann_window(WINDOW_LENGTH))

    def forward(self, waveform):
        stft = torch.stft(
            waveform.squeeze(1),
            n_fft=N_FFT,
            hop_length=HOP_LENGTH,
            win_length=WINDOW_LENGTH,
            window=self.window,
            return_complex=True,
        )
        stft_in = torch.stack([stft.real, stft.imag], dim=1)
        stft_out = self.unet(stft_in)
        out_complex = torch.complex(stft_out[:, 0], stft_out[:, 1])

        denoised = torch.istft(
            out_complex,
            n_fft=N_FFT,
            hop_length=HOP_LENGTH,
            win_length=WINDOW_LENGTH,
            window=self.window,
            length=waveform.shape[-1],
        )

        return denoised.unsqueeze(1), out_complex
</code></pre>

<div class="figure_container">
			<a data-fancybox="gallery0" href="media/2channel_output/denoise_compare.png">
				<img class="figure" src="media/2channel_output/denoise_compare.png"></img>
			</a>
		</div>
		<div class="figure_caption">
			<span class="caption_title">Figure:</span> Example 
		</div>

		<p>
			Text
		</p>

		<div class="figure_container">
			<audio style="width: 32%;" controls>
				<source src="media/2channel_output/clean.wav" type="audio/wav">
				Your browser does not support the audio element.
			</audio>
			<audio style="width: 33%;" controls>
				<source src="media/2channel_output/noisy.wav" type="audio/wav">
				Your browser does not support the audio element.
			</audio>
			<audio style="width: 33%;" controls>
				<source src="media/2channel_output/denoised.wav" type="audio/wav">
				Your browser does not support the audio element.
			</audio>
		</div>
		<div class="figure_caption">
			<span class="caption_title">Figure:</span> Clean (left), noisy (middle) denoised (right).
		</div>



	</section>
	
	<link rel="stylesheet" href="../fancybox.css" />
	<script src="../fancybox.umd.js"></script>

	<script>
		Fancybox.bind("[data-fancybox]", {
			closeButton: false,
		});
	  </script>

</body>
</html>
