<!DOCTYPE html>
<html>
<head>
	<title>Five-minute LLMs</title>

	<meta name="viewport" content="width=device-width, initial-scale=1">
	<meta name="theme-color" content="#FBEDEA" />

	<link rel="shortcut icon" type="image/x-icon"  href="../../favicon.ico?">
	<link rel="apple-touch-icon" href="../../apple-touch-icon.png">

	<link rel="stylesheet" href="style.css">

	<script>
	window.MathJax = {
	tex: {
		inlineMath: [['$', '$'], ['\\(', '\\)']]
	}
	};
	</script>

	<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

	<link rel="preconnect" href="https://fonts.googleapis.com">
	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>

	<link href="https://fonts.googleapis.com/css2?family=Roboto+Mono:wght@400;500;600;700&display=swap" rel="stylesheet">

	<link rel="stylesheet" href="../atom-one-dark.min.css">
	<script src="../highlight.min.js"></script>

	<script>hljs.highlightAll();</script>

	<style>
		.audio-title {
			text-align: center;
			font-weight: 900;
			width: 100%;
			font-size: 24px;
			font-family: 'Roboto Mono', monospace;
		}

		audio {
			display: block;
			margin-left: auto;
			margin-right: auto;
			margin-top: 4px;
			margin-bottom: 10px;
		}
		
		.result_group img {
			width: 100%;
			margin-bottom: 36px;
		}
	</style>

</head>
<body>
	<div class="blog centering" id="back_container">
		<a href="../index.html" class="back"><svg xmlns="http://www.w3.org/2000/svg" width="18" height="14" fill="currentColor" class="bi bi-caret-left" viewBox="0 0 22 18">
			<path fill-rule="evenodd" clip-rule="evenodd" d="M14.0303 7.46967C14.3232 7.76256 14.3232 8.23744 14.0303 8.53033L10.5607 12L14.0303 15.4697C14.3232 15.7626 14.3232 16.2374 14.0303 16.5303C13.7374 16.8232 13.2626 16.8232 12.9697 16.5303L8.96967 12.5303C8.67678 12.2374 8.67678 11.7626 8.96967 11.4697L12.9697 7.46967C13.2626 7.17678 13.7374 7.17678 14.0303 7.46967Z"/>
		  </svg>All posts</a>
	</div>

	<section class="blog centering post">
		<h1>Five-minute LLMs</h1>
		<div class="subtitle monospace">By Simon Halvdansson&nbsp;|&nbsp;Sep. 2025</div>

		<hr class="squiggly-line"/>

		<p>
			Over the past few years there has been many innovations in LLM design; smart pretraining recipies, substantial posttraining RL, and architectural improvements. In this post, we'll look at how recent architectural improvements translate to very small models trained for short amounts of time. This means that we are not optimizing for low parameter count, optimal there would be something like <a href="https://developers.googleblog.com/en/introducing-gemma-3-270m/" target="_blank">Gemma 3 270M</a>. We are also not interested in distillation nor quantization. Instead, we are interested in the Pareto frontier in the setting where we can only train the model for five minutes on a specific data set. The recent post <a href="https://www.seangoedecke.com/model-on-a-mbp/">What's the strongest AI model you can train on a laptop in five minutes?</a> by Sean Goedecke inspired this post and in this post we aim to go deeper in terms of optimizing the architecture and doing proper A/B testing.
		</p>

		<div class="figure_container_small">
			<a data-fancybox="gallery0" href="media/beepboop_spectrogram_progress.gif">
				<img class="figure" src="media/beepboop_spectrogram_progress.gif"></img>
			</a>
		</div>
		<div class="figure_caption">
			<span class="caption_title">Figure: </span> Want a scatter plot of preplexity for different models here.
		</div>

		<p>
			Following that post, we will restrict ourselves to the <a href="https://arxiv.org/abs/2305.07759" target="_blank">TinyStories</a> dataset and conider validation preplexity as the metric which we are ultimately optimizing for. We will start from a GPT-2 style model and look at the effect of the following choices:
			<ul>
				<li>Learning rate</li>
				<li>Optimizer (Muon)</li>
				<li>Parameters (Hidden dimension, layers, heads)</li>
				<li>FFN: MLP/SwiGLU</li>
				<li>Dropout</li>
				<li>Pre/post norm</li>
				<li>LayerNorm / RMSNorm</li>
				<li>Grouped Query Attention (GQA)</li>
			</ul>
			Before starting, we'll say a couple of things about modern archtectures and the methodology. If you just want the results, jump to them <a href="#results">here</a>.
		</p>

		<hr class="squiggly-line"/>
		
		<h2>Modern models</h2>

		<p>
			
			The excellent <a href="https://magazine.sebastianraschka.com/p/from-gpt-2-to-gpt-oss-analyzing-the" target="_blank">overview</a> by Sebastian Raschka.
		</p>

		<hr class="squiggly-line"/>

		<h2>Methodology</h2>

		<p>
			We will train all models with automatic mixed precision (<a href="https://docs.pytorch.org/docs/stable/amp.html">AMP</a>) and <code>bfloat16</code> since this generally doubles throughout at the cost of some training stability which is preferable in this time-constrained setting.
		</p>

		<hr class="squiggly-line"/>

		<h2 id="results">Results</h2>
		
		<p>text</p>
		


	</section>
	
	<link rel="stylesheet" href="../fancybox.css" />
	<script src="../fancybox.umd.js"></script>

	<script>
		Fancybox.bind("[data-fancybox]", {
			closeButton: false,
		});
	  </script>


</body>
</html>