<!DOCTYPE html>
<html>
<head>
	<title>Five-minute LLMs</title>

	<meta name="viewport" content="width=device-width, initial-scale=1">
	<meta name="theme-color" content="#FBEDEA" />

	<link rel="shortcut icon" type="image/x-icon"  href="../../favicon.ico?">
	<link rel="apple-touch-icon" href="../../apple-touch-icon.png">

	<link rel="stylesheet" href="style.css">

	<script>
	window.MathJax = {
	tex: {
		inlineMath: [['$', '$'], ['\\(', '\\)']]
	}
	};
	</script>

	<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

	<link rel="preconnect" href="https://fonts.googleapis.com">
	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>

	<link href="https://fonts.googleapis.com/css2?family=Roboto+Mono:wght@400;500;600;700&display=swap" rel="stylesheet">

	<link rel="stylesheet" href="../atom-one-dark.min.css">
	<script src="../highlight.min.js"></script>

	<script>hljs.highlightAll();</script>

	<style>
		.audio-title {
			text-align: center;
			font-weight: 900;
			width: 100%;
			font-size: 24px;
			font-family: 'Roboto Mono', monospace;
		}

		audio {
			display: block;
			margin-left: auto;
			margin-right: auto;
			margin-top: 4px;
			margin-bottom: 10px;
		}
		
		.result_group img {
			width: 100%;
			margin-bottom: 36px;
		}
	</style>

</head>
<body>
	<div class="blog centering" id="back_container">
		<a href="../index.html" class="back"><svg xmlns="http://www.w3.org/2000/svg" width="18" height="14" fill="currentColor" class="bi bi-caret-left" viewBox="0 0 22 18">
			<path fill-rule="evenodd" clip-rule="evenodd" d="M14.0303 7.46967C14.3232 7.76256 14.3232 8.23744 14.0303 8.53033L10.5607 12L14.0303 15.4697C14.3232 15.7626 14.3232 16.2374 14.0303 16.5303C13.7374 16.8232 13.2626 16.8232 12.9697 16.5303L8.96967 12.5303C8.67678 12.2374 8.67678 11.7626 8.96967 11.4697L12.9697 7.46967C13.2626 7.17678 13.7374 7.17678 14.0303 7.46967Z"/>
		  </svg>All posts</a>
	</div>

	<section class="blog centering post">
		<h1>Five-minute SLMs</h1>
		<div class="subtitle monospace">By Simon Halvdansson&nbsp;|&nbsp;Sep. 2025</div>

		<hr class="squiggly-line"/>

		<p>
			Over the past few years there has been many innovations in LLM design; smart pretraining recipes, substantial posttraining RL, and architectural improvements. In this post, we'll look at how some of the architectural improvements translate to very small models trained for short amounts of time. This means that we are not optimizing for low parameter count, optimal there would be something like <a href="https://developers.googleblog.com/en/introducing-gemma-3-270m/" target="_blank">Gemma 3 270M</a>. We are also not interested in distillation nor quantization. Instead, we are interested in the optimal design choices when training a model for only five minutes with consumer hardware on a smaller data set. The recent exploration <a href="https://www.seangoedecke.com/model-on-a-mbp/">What's the strongest AI model you can train on a laptop in five minutes?</a> by Sean Goedecke inspired this post and here we will aim to go deeper in terms of optimizing the architecture and doing proper A/B testing. The ultimate form for this type of investigation is <a href="https://github.com/KellerJordan/modded-nanogpt">Modded-NanoGPT</a> which is considerably more hardcore but less accessible.
		</p>

		<div class="figure_container_small">
			<a data-fancybox="gallery0" href="media/beepboop_spectrogram_progress.gif">
				<img class="figure" src="media/beepboop_spectrogram_progress.gif"></img>
			</a>
		</div>
		<div class="figure_caption">
			<span class="caption_title">Figure: </span> We'll see what we put here.
		</div>

		<p>
			Following the post by Goedecke, we will restrict ourselves to the <a href="https://arxiv.org/abs/2305.07759" target="_blank">TinyStories</a> dataset and pick validation cross-entropy as the metric which we are ultimately optimizing for. We will start from a GPT-2 style model and look at the effect of the following choices:
			<ul>
				<li>Learning rate</li>
				<li>Schedule-Free optimizer</li>
				<li>Model sizing (hidden dimension, layers, heads)</li>
				<li>Gradient clipping</li>
				<li>FFN: MLP/SwiGLU</li>
				<li>Dropout</li>
				<li>Pre/post norm</li>
				<li>LayerNorm/RMSNorm</li>
				<li>Sinusoidal/learnable positional embeddings</li>
			</ul>
			Before running evaluations, we'll say a couple of things about modern architectures and the methodology.
		</p>

		<hr class="squiggly-line"/>
		
		<h2>What we change</h2>

		<p>
			This excellent <a href="https://magazine.sebastianraschka.com/p/from-gpt-2-to-gpt-oss-analyzing-the" target="_blank">overview</a> by Sebastian Raschka looks at some of the ways modern models differ from the classical GPT-2 model. There is an overwhelming amount of papers suggesting various changes to the base transformer architecture, and through the test of time what is considered "standard" has evolved considerably. Below we briefly outline some of those developments which we vary in this post. 
		</p>

		<h4>Schedule-Free</h4>
		<p>
			Introduced in the 2024 <a href="https://arxiv.org/abs/2405.15682" target="_blank">The Road Less Scheduled</a> paper, the Schedule-Free optimizer from Meta promises to outperform the standard AdamW optimizer without the need for a complicated cosine scheduler. Removing the need for a scheduler or, alternatively, getting some of the benefits of a scheduler without the need to add additional hyperparameters sounds like a very nice deal. I've used this for some projects before and the results in the paper for a ResNet and an LLM are promising so I wanted to try it out here.
		</p>
		<p>
			Usage is quite simple but not quite a drop-in replacement. Initialization is standard but we need to keep track of an internal <code>train/eval</code> mode just like we toggle for for model during training/inference.

		</p>

<pre><code class="language-py">import schedulefree

...

if args["optimizer"] == "adamw":
	optimizer = torch.optim.AdamW(model.parameters(), lr=args["lr"])
else:
	optimizer = schedulefree.AdamWScheduleFree(model.parameters(), lr=args["lr"])

...

if args["optimizer"] == "schedulefree" and optimizer is not None:
	if train:
		optimizer.train()
	else:
		optimizer.eval()
</code></pre>
		<h4>Gradient clipping</h4>
		<p>
			Gradient clipping is a popular techniques to stabilize training by clipping large gradients, avoiding particularly large gradients which may steer training off its course. In between <code>loss.backward()</code> and <code>optimizer.step()</code> we simply call <code>nn.utils.clip_grad_norm_(model.parameters(), args["grad_clip"])</code>
		</p>
		<h4>SwiGLU</h4>
		<p>
			In the latter part of a transformer block is a feedforward-network (FFN) which originally took the form of a standard multilayer perceptron (MLP) with ReLU activation. In the years since, the Swish (also called SiLU) activation functin has grown in popularity and the FFN has been modified to incorporate Gated Linear Units (GLUs). The original paper <a href="https://arxiv.org/abs/2002.05202" target="_blank">GLU Variants Improve Transformer</a> by Noam Shazeer contains extensive tests on older architectures but modern LLMs seem to all include this modification.
		</p>
<pre><code class="language-py">class SwiGLU(nn.Module):
    def __init__(self, d_model: int, hidden: int, dropout: float):
        super().__init__()
        self.fc1 = nn.Linear(d_model, 2 * hidden)
        self.fc2 = nn.Linear(hidden, d_model)
        self.drop = nn.Dropout(dropout)

    def forward(self, x):
        u, v = self.fc1(x).chunk(2, dim=-1)      # [*, 2*hidden] -> two [*, hidden]
        x = F.silu(u) * v                         # SwiGLU: swish(u) ⊙ v
        x = self.fc2(x)
        return self.drop(x)
</code></pre>

		<h4>Dropout</h4>
		<p>
			Dropout is a very old (by modern standards) regularization technique. I feel like I've read papers both positive and negative about dropout in LLMs but the prevaling notion seems to be that it is superflous, especially since we rarely train for high numbers of epochs in todays era of high parameter counts.
		</p>
		<h4>RMSNorm</h4>
		<p>
			LayerNorm is the standard solution for normalizing the state tensor over the feature dimension while going through transformer blocks by subtracting the mean, dividing by the standard deviation and applying a learned affine map. In RMSNorm, we skip the subtracting of the mean, allowing activations to drift more. As I've understood it, this minor change slightly improves throughput while having an even lesser effect on performance of the models. Both LayerNorm and RMSNorm are standard enough nowadays to be included in PyTorch as <code>nn.LayerNorm</code> and <code>nn.RMSNorm</code>.
		</p>
		<h4>Pre/post norm</h4>
		<p>
			Vanishing gradients has for a long time been a problem for deep networks. Apart from skip connections, the original Vaswani transformer paper also included two normalization layers inside each transformer block to stabilize training. These normalizations were placed after the attention and feedforward-networks but later work has largely placed this part <i>before</i> each sub-block. 
		</p>

		<h4>Sinusoidal/learnable positional embeddings</h4>
		<p>
			Sinudoidal positional embeddings were used in the original transformer paper but was quickly replaced by learned learned embeddings and later Rotary Positional Embeddings (RoPE). While RoPE acts directly on the <code>Q</code> and <code>K</code> matrices, sinusoidal and learned positional embeddings simply take the form of adding a vector to each token embedding making them a bit easier to implement.
		</p>

		<hr class="squiggly-line"/>

		<h2>Methodology</h2>

		<p>
			We will train all models with automatic mixed precision (<a href="https://docs.pytorch.org/docs/stable/amp.html">AMP</a>) and <code>bfloat16</code> since this generally doubles throughput at the cost of some training stability which is preferable in this time-constrained setting. Searching over all possible configurations is outside my time and electricity budget so we will assume that all choices contribute to the result independently.
		</p>

<pre><code class="language-py">args = {
		"lr":			2e-3,
        "optimizer":    "adamw",
        "n_layer":      2,
        "n_head":       2,
        "d_model":      128,
        "dropout":      0.0,
        "grad_clip":    None,
        "norm":         "layer",
        "ffn":          "mlp",
        "prepost":      "pre",
        "pos_emb":      "sinusoidal"
}
</code></pre>

		<p>
			The performance of our models will have natural variation. To be able to conduct meaningful A/B experiments, we need to have a model for how our metric naturally varies. We can investigate this manually by constructing a histogram of validation cross-entropy for our default configuration.
		</p>

		<div class="figure_container_small">
			<a data-fancybox="gallery1" href="media/histogram_baseline.png">
				<img class="figure" src="media/histogram_baseline.png"></img>
			</a>
		</div>
		<div class="figure_caption">
			<span class="caption_title">Figure: </span> Histogram of validation cross-entropy for our default configuration for 100 samples, together with (poor) Gaussian fit.
		</div>

		<p>
			As we (hopefully) see in the figure, this distribution is at least somewhat Gaussian. For a given configuration, we will want to estimate the mean of the probability distribution which validation cross-entropy for models trained with this distribution follows. Since we do not know the variance, we will model our estimate for the true mean using the <a target="_blank" href="https://en.wikipedia.org/wiki/Student%27s_t-distribution">Student's $t$-distribution</a>. Using this distribution, we can get confidence intervals for the true mean as
			$$
			\bar{x} \pm t_{1-\alpha/2,\, n-1} \frac{s}{\sqrt{n}}
			$$
			where $\bar{x}$ is the sample mean, $s$ is the sample standard deviation, $n$ is the number of samples and $\alpha$ is the parameter for the confidence interval.
		</p>

		<p>
			From here, we will tweak the architecture to beat this base validation loss of around <code>2.15</code>. Going over <b>all</b> possible configurations is simple infeasible, even on a 5 minute time budget, without significant parallell compute. Instead, we will work through the hyperparamters in a semi-heuristical way, tweaking the most important parameters first.
		</p>

		<hr class="squiggly-line"/>

		<h2 id="results">Hyperparameter search</h2>
		
		<p>
			As our first course of action, we will sweep over the learning rate between <code>1e-4</code> and <code>1e-2</code> for both AdamW and Schedule-Free AdamW since this hyperparameter can have a large effect. Doing this we find the following behavior:
		</p>

		<div class="figure_container_small">
			<a data-fancybox="gallery3" href="media/lr_sweep_baseline.png">
				<img class="figure" src="media/lr_sweep_baseline.png"></img>
			</a>
		</div>
		<div class="figure_caption">
			<span class="caption_title">Figure: </span> Sweep over feasible learning rates for both AdamW and Schedule-Free with the default configuration.
		</div>

		<p>
			Based on this information, we set our configuration's learning rate to <code>2e-3</code> and stick with the standard <code>AdamW</code> optimizer. Next up we tweak the scaling hyperparameters for the model, the number of layers <code>n_layers</code>, the number of heads <code>n_heads</code> and the hidden model dimension <code>d_model</code>. Tweaking all of these at the same time for four different options for each hyperparameters means going over <code>4 × 4 × 4 = 64</code> configurations with several runs for each to develop confidence intervals.
		</p>

		<div class="figure_container">
			<a data-fancybox="gallery2" href="media/heatmap_ldh_baseline.png">
				<img class="figure" src="media/heatmap_ldh_baseline.png"></img>
			</a>
		</div>
		<div class="figure_caption">
			<span class="caption_title">Figure: </span> Results for all hyperparameter choices for number of layers, heads and model dimension.
		</div>

		<p>
			Running these sweeps is time intensive due to the large search space so we only perform 7 training runs per configuration. Still we can see continuity in the heatmaps above.
		</p>

		<div class="figure_container">
			<a data-fancybox="gallery2" href="media/arch_sweep_baseline.png">
				<img class="figure" src="media/arch_sweep_baseline.png"></img>
			</a>
		</div>
		<div class="figure_caption">
			<span class="caption_title">Figure: </span> Layers, heads and model dimension results ordered in decreasing performance together with 95% confidence intervals.
		</div>

		<p>
			Note that we cannot say with any $p < 0.05$ confidence that the <code>n_layers=2</code>, <code>n_heads = 4</code>, <code>d_model = 256</code> is the global minimum in our search space but it is clear from the heatmaps that it at least is very close to the global minimum and so we set it in our configuration for now. Next up we compare <code>RMSNorm</code> and <code>LayerNorm</code>.
		</p>
		
		<div class="figure_container_small">
			<a data-fancybox="gallery4" href="media/binary_norm_done.png">
				<img class="figure" src="media/binary_norm_done.png"></img>
			</a>
		</div>
		<div class="figure_caption">
			<span class="caption_title">Figure: </span> Comparision of validation loss for LayerNorm compared to RMSNorm. Notice the scale on the y axis.
		</div>

		<p>
			As we see in the figure, there is no clear winner despite this being the averages over 80 training runs per configuration. This is however to be expected as the optimization in <code>RMSNorm</code> is so small that it should only make a difference for much larger networks. Nevertheless, it is good to know that its simplification does not derail training for small sizes and training times.
		</p>
		
		<div class="figure_container_small">
			<a data-fancybox="gallery4" href="media/binary_ffn_done.png">
				<img class="figure" src="media/binary_ffn_done.png"></img>
			</a>
		</div>
		<div class="figure_caption">
			<span class="caption_title">Figure: </span> Comparision of validation loss for a standard MLP and SwiGLU for the feedforward netwrok path of the transformer.
		</div>

		<p>
			For the <code>MLP</code> vs <code>SwiGLU</code> comparision the result is a bit more surprising.
		</p>
		
		<div class="figure_container_small">
			<a data-fancybox="gallery4" href="media/binary_prepost_big.png">
				<img class="figure" src="media/binary_prepost_big.png"></img>
			</a>
		</div>
		<div class="figure_caption">
			<span class="caption_title">Figure: </span> Comparision of validation loss for pre-norm and post-norm.
		</div>

		<div class="figure_container_small">
			<a data-fancybox="gallery4" href="media/binary_pos_emb_done.png">
				<img class="figure" src="media/binary_pos_emb_done.png"></img>
			</a>
		</div>
		<div class="figure_caption">
			<span class="caption_title">Figure: </span> Comparision between sinusoidal and learned positional embeddings.
		</div>



		<div class="figure_container_small">
			<a data-fancybox="gallery4" href="media/bars_dropout.png">
				<img class="figure" src="media/bars_dropout.png"></img>
			</a>
		</div>
		<div class="figure_caption">
			<span class="caption_title">Figure: </span> Sweep over feasible dropout probabilities for use in training.
		</div>

		<div class="figure_container_small">
			<a data-fancybox="gallery4" href="media/bars_grad_clip.png">
				<img class="figure" src="media/bars_grad_clip.png"></img>
			</a>
		</div>
		<div class="figure_caption">
			<span class="caption_title">Figure: </span> Comparision between Gradient clipping options for use in training.
		</div>


		

		
		


	</section>
	
	<link rel="stylesheet" href="../fancybox.css" />
	<script src="../fancybox.umd.js"></script>

	<script>
		Fancybox.bind("[data-fancybox]", {
			closeButton: false,
		});
	  </script>


</body>
</html>