<!DOCTYPE html>
<html>
<head>
	<title>Five-minute LLMs</title>

	<meta name="viewport" content="width=device-width, initial-scale=1">
	<meta name="theme-color" content="#FBEDEA" />

	<link rel="shortcut icon" type="image/x-icon"  href="../../favicon.ico?">
	<link rel="apple-touch-icon" href="../../apple-touch-icon.png">

	<link rel="stylesheet" href="style.css">

	<script>
	window.MathJax = {
	tex: {
		inlineMath: [['$', '$'], ['\\(', '\\)']]
	}
	};
	</script>

	<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

	<link rel="preconnect" href="https://fonts.googleapis.com">
	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>

	<link href="https://fonts.googleapis.com/css2?family=Roboto+Mono:wght@400;500;600;700&display=swap" rel="stylesheet">

	<link rel="stylesheet" href="../atom-one-dark.min.css">
	<script src="../highlight.min.js"></script>

	<script>hljs.highlightAll();</script>

	<style>
		.audio-title {
			text-align: center;
			font-weight: 900;
			width: 100%;
			font-size: 24px;
			font-family: 'Roboto Mono', monospace;
		}

		audio {
			display: block;
			margin-left: auto;
			margin-right: auto;
			margin-top: 4px;
			margin-bottom: 10px;
		}
		
		.result_group img {
			width: 100%;
			margin-bottom: 36px;
		}
	</style>

</head>
<body>
	<div class="blog centering" id="back_container">
		<a href="../index.html" class="back"><svg xmlns="http://www.w3.org/2000/svg" width="18" height="14" fill="currentColor" class="bi bi-caret-left" viewBox="0 0 22 18">
			<path fill-rule="evenodd" clip-rule="evenodd" d="M14.0303 7.46967C14.3232 7.76256 14.3232 8.23744 14.0303 8.53033L10.5607 12L14.0303 15.4697C14.3232 15.7626 14.3232 16.2374 14.0303 16.5303C13.7374 16.8232 13.2626 16.8232 12.9697 16.5303L8.96967 12.5303C8.67678 12.2374 8.67678 11.7626 8.96967 11.4697L12.9697 7.46967C13.2626 7.17678 13.7374 7.17678 14.0303 7.46967Z"/>
		  </svg>All posts</a>
	</div>

	<section class="blog centering post">
		<h1>Thirty seconds to SLMs</h1>
		<div class="subtitle monospace">By Simon Halvdansson&nbsp;|&nbsp;Sep. 2025</div>

		<hr class="squiggly-line"/>

		<p>
			Over the past few years there has been many innovations in LLM design; smart pretraining recipes, substantial posttraining RL, and architectural improvements. In this post, we'll look at how recent architectural improvements translate to very small models trained for short amounts of time. This means that we are not optimizing for low parameter count, optimal there would be something like <a href="https://developers.googleblog.com/en/introducing-gemma-3-270m/" target="_blank">Gemma 3 270M</a>. We are also not interested in distillation nor quantization. Instead, we are interested in the optimal design choices when training a model for only five minutes with consumer hardware on a smaller data set. The recent exploration <a href="https://www.seangoedecke.com/model-on-a-mbp/">What's the strongest AI model you can train on a laptop in five minutes?</a> by Sean Goedecke inspired this post and here we will aim to go deeper in terms of optimizing the architecture and doing proper A/B testing. The ultimate form for this type of investigation is <a href="https://github.com/KellerJordan/modded-nanogpt">Modded-NanoGPT</a> which is considerably more hardcore but less accessible.
		</p>

		<div class="figure_container_small">
			<a data-fancybox="gallery0" href="media/beepboop_spectrogram_progress.gif">
				<img class="figure" src="media/beepboop_spectrogram_progress.gif"></img>
			</a>
		</div>
		<div class="figure_caption">
			<span class="caption_title">Figure: </span> We'll see what we put here.
		</div>

		<p>
			Following the post by Goedecke, we will restrict ourselves to the <a href="https://arxiv.org/abs/2305.07759" target="_blank">TinyStories</a> dataset and pick validation cross-entropy as the metric which we are ultimately optimizing for. We will start from a GPT-2 style model and look at the effect of the following choices:
			<ul>
				<li>Learning rate</li>
				<li>Schedule-Free optimizer</li>
				<li>Model sizing (hidden dimension, layers, heads)</li>
				<li>Gradient clipping</li>
				<li>FFN: MLP/SwiGLU</li>
				<li>Dropout</li>
				<li>Pre/post norm</li>
				<li>LayerNorm / RMSNorm</li>
				<li>Sinusoidal/learnable positional embeddings?</li>
			</ul>
			Before starting, we'll say a couple of things about modern architectures and the methodology. If you just want the results, jump to them <a href="#results">here</a>.
		</p>

		<hr class="squiggly-line"/>
		
		<h2>Modern models</h2>

		<p>
			This excellent <a href="https://magazine.sebastianraschka.com/p/from-gpt-2-to-gpt-oss-analyzing-the" target="_blank">overview</a> by Sebastian Raschka looks at some of the ways modern models differ from the classical GPT-2 model. There is an overwhelming amount of papers suggesting various changes to the base transformer architecture, and through the test of time what is considered "standard" has evolved considerably. Below we briefly outline some of those developments which we vary in this post. 
		</p>

		<h4>Schedule-Free</h4>
		<p>
		Introduced in the 2024 <a href="https://arxiv.org/abs/2405.15682" target="_blank">The Road Less Scheduled</a> paper, the Schedule-Free optimizer from Meta promises to outperform the standard AdamW optimizer without the need for a complicated cosine scheduler. On the face of it 
		</p>
		<h4>Gradient clipping</h4>
		<p>
			Text
		</p>
		<h4>SwiGLU</h4>
		<p>
			Text
		</p>
		<h4>Pre/post norm</h4>
		<p>
			Text
		</p>
		<h4>RMSNorm</h4>
		<p>
			Text
		</p>
		<h4>Sinusoidal/learnable positional embeddings</h4>
		<p>
			Text
		</p>

		<hr class="squiggly-line"/>

		<h2>Methodology</h2>

		<p>
			We will train all models with automatic mixed precision (<a href="https://docs.pytorch.org/docs/stable/amp.html">AMP</a>) and <code>bfloat16</code> since this generally doubles throughput at the cost of some training stability which is preferable in this time-constrained setting. Searching over all possible configurations is outside my time and electricity budget so we will assume that all choices contribute to the result independently.
		</p>

		<p>
			The performance of our models will have natural variation. To be able to conduct meaningful A/B experiments, we need to have a model for how our metric naturally varies. We can investigate this manually by constructing a histogram of validation cross-entropy for our default configuration. TODO: INSERT DEFAULT CONFIGURATION HERE
		</p>

		<div class="figure_container_small">
			<a data-fancybox="gallery1" href="media/histogram.png">
				<img class="figure" src="media/histogram.png"></img>
			</a>
		</div>
		<div class="figure_caption">
			<span class="caption_title">Figure: </span> Histogram of validation cross-entropy for our default configuration.
		</div>

		<p>
			As we (hopefully) see in the figure, this distribution is at least somewhat Gaussian. For a given configuration, we will want to estimate the mean of the probability distribution which validation cross-entropy for models trained with this distribution follows. Since we do not know the variance, we will model our estimate for the true mean using the <a target="_blank" href="https://en.wikipedia.org/wiki/Student%27s_t-distribution">Student's $t$-distribution</a>. Using this distribution, we can get confidence intervals for the true mean as
			$$
			\bar{x} \pm t_{1-\alpha/2,\, n-1} \frac{s}{\sqrt{n}}
			$$
			where $\bar{x}$ is the sample mean, $s$ is the sample standard deviation, $n$ is the number of samples and $\alpha$ is the parmeter for the confidence interval.
		</p>

		<hr class="squiggly-line"/>

		<h2 id="results">Results</h2>
		
		<p>text</p>

		<div class="figure_container">
			<a data-fancybox="gallery2" href="media/heatmap_ldh.png">
				<img class="figure" src="media/heatmap_ldh.png"></img>
			</a>
		</div>
		<div class="figure_caption">
			<span class="caption_title">Figure: </span> Results for all hyperparameter choices for number of layers, heads and model dimension.
		</div>

		<div class="figure_container">
			<a data-fancybox="gallery2" href="media/arch_sweep.png">
				<img class="figure" src="media/arch_sweep.png"></img>
			</a>
		</div>
		<div class="figure_caption">
			<span class="caption_title">Figure: </span> Layers, heads and model dimension results ordered in decreasing performance together with 95% confidence intervals.
		</div>

		<div class="figure_container_small">
			<a data-fancybox="gallery3" href="media/lr_sweep.png">
				<img class="figure" src="media/lr_sweep.png"></img>
			</a>
		</div>
		<div class="figure_caption">
			<span class="caption_title">Figure: </span> Sweep over feasible learning rates for both AdamW and Schedule-Free.
		</div>

		
		<div class="figure_container_small">
			<a data-fancybox="gallery4" href="media/binary_norm.png">
				<img class="figure" src="media/binary_norm.png"></img>
			</a>
		</div>
		<div class="figure_caption">
			<span class="caption_title">Figure: </span> Comparision of validation loss for LayerNorm compared to RMSNorm.
		</div>

		
		<div class="figure_container_small">
			<a data-fancybox="gallery4" href="media/binary_ffn.png">
				<img class="figure" src="media/binary_ffn.png"></img>
			</a>
		</div>
		<div class="figure_caption">
			<span class="caption_title">Figure: </span> Comparision of validation loss for a standard MLP and SwiGLU for the feedforward netwrok path of the transformer.
		</div>

		
		<div class="figure_container_small">
			<a data-fancybox="gallery4" href="media/binary_prepost.png">
				<img class="figure" src="media/binary_prepost.png"></img>
			</a>
		</div>
		<div class="figure_caption">
			<span class="caption_title">Figure: </span> Comparision of validation loss for pre-norm and post-norm.
		</div>

		<div class="figure_container_small">
			<a data-fancybox="gallery4" href="media/binary_pos_emb.png">
				<img class="figure" src="media/binary_pos_emb.png"></img>
			</a>
		</div>
		<div class="figure_caption">
			<span class="caption_title">Figure: </span> Comparision between sinusoidal and learned positional embeddings.
		</div>



		<div class="figure_container_small">
			<a data-fancybox="gallery4" href="media/bars_dropout.png">
				<img class="figure" src="media/bars_dropout.png"></img>
			</a>
		</div>
		<div class="figure_caption">
			<span class="caption_title">Figure: </span> Sweep over feasible dropout probabilities for use in training.
		</div>

		<div class="figure_container_small">
			<a data-fancybox="gallery4" href="media/bars_grad_clip.png">
				<img class="figure" src="media/bars_grad_clip.png"></img>
			</a>
		</div>
		<div class="figure_caption">
			<span class="caption_title">Figure: </span> Comparision between Gradient clipping options for use in training.
		</div>


		

		
		


	</section>
	
	<link rel="stylesheet" href="../fancybox.css" />
	<script src="../fancybox.umd.js"></script>

	<script>
		Fancybox.bind("[data-fancybox]", {
			closeButton: false,
		});
	  </script>


</body>
</html>