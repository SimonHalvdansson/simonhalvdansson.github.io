<!DOCTYPE html>
<html>
<head>
	<title>Neural spectrogram phase retrieval with SIRENs</title>

	<meta name="viewport" content="width=device-width, initial-scale=1">
	<meta name="theme-color" content="#FBEDEA" />

	<link rel="shortcut icon" type="image/x-icon"  href="../../favicon.ico?">
	<link rel="apple-touch-icon" href="../../apple-touch-icon.png">

	<link rel="stylesheet" href="style.css">

	<script>
	window.MathJax = {
	tex: {
		inlineMath: [['$', '$'], ['\\(', '\\)']]
	}
	};
	</script>

	<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

	<link rel="preconnect" href="https://fonts.googleapis.com">
	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>

	<link href="https://fonts.googleapis.com/css2?family=Roboto+Mono:wght@400;500;600;700&display=swap" rel="stylesheet">

	<link rel="stylesheet" href="../atom-one-dark.min.css">
	<script src="../highlight.min.js"></script>

	<script>hljs.highlightAll();</script>

	<style>
		.audio-title {
			text-align: center;
			font-weight: 900;
			width: 100%;
			font-size: 24px;
			font-family: 'Roboto Mono', monospace;
		}

		audio {
			display: block;
			margin-left: auto;
			margin-right: auto;
			margin-top: 4px;
			margin-bottom: 10px;
		}
		
		.result_group img {
			width: 100%;
			margin-bottom: 36px;
		}
	</style>

</head>
<body>
	<div class="blog centering" id="back_container">
		<a href="../index.html" class="back"><svg xmlns="http://www.w3.org/2000/svg" width="18" height="14" fill="currentColor" class="bi bi-caret-left" viewBox="0 0 22 18">
			<path fill-rule="evenodd" clip-rule="evenodd" d="M14.0303 7.46967C14.3232 7.76256 14.3232 8.23744 14.0303 8.53033L10.5607 12L14.0303 15.4697C14.3232 15.7626 14.3232 16.2374 14.0303 16.5303C13.7374 16.8232 13.2626 16.8232 12.9697 16.5303L8.96967 12.5303C8.67678 12.2374 8.67678 11.7626 8.96967 11.4697L12.9697 7.46967C13.2626 7.17678 13.7374 7.17678 14.0303 7.46967Z"/>
		  </svg>All posts</a>
	</div>

	<section class="blog centering post">
		<h1>Neural spectrogram phase retrieval with SIRENs</h1>
		<div class="subtitle monospace">By Simon Halvdansson&nbsp;|&nbsp;Aug. 2025</div>

		<hr class="squiggly-line"/>

		<p>
			Modifying or producing spectrograms instead of working directly on a signal or short-time Fourier transform is powerful for applications such as phase vocoders, source separation, masking and TTS systems because you have a low-dimensional representation of your signal. Going from a spectrogram to an actual signal is the process of <i>phase retrieval</i> and is a well-studied problem. Many algorithms and approaches have been proposed over the last few decades and in this post we will propose a very simple yet highly effective one based on <a href="../gabor-feature-experiments/index.html" target="_blank">implicit neural representations</a> (INRs) and more specifically the <a href="https://www.vincentsitzmann.com/siren/" target="_blank">SIREN</a> architecture by Sitzmann et al. from 2020.
		</p>

		<div class="figure_container_small">
			<a data-fancybox="gallery0" href="media/beepboop_spectrogram_progress.gif">
				<img class="figure" src="media/beepboop_spectrogram_progress.gif"></img>
			</a>
		</div>
		<div class="figure_caption">
			<span class="caption_title">Figure: </span> Learning process of a SIREN network in spectrograms.
		</div>

		<p>
			The SIREN architecture is (up to minor details) just a standard MLP with sine activation functions in each layer which has been shown to be very parameter-efficient for approximating real-world signals such as audio, images, videos and 3D models. In the original paper, the authors show that the quantity
		</p>
		<div class="equation">
			$\sum_i |\mathcal{N}(i) - y(i)|^2,$
		</div>
		<p>
			where $\mathcal{N}$ is a SIREN neural network and $y$ is some signal, can be minimized to a smaller loss with fewer parameters and fewer steps than other comparable models. Note of course that any universal function approximator can minimize this quantity but it is the effectiveness of SIRENs which is remarkable. 
		</p>

		<p>
			The idea of using some implicit neural representation to parametrize the target signal for phase retrieval is not new, at least for non-spectrogram phase retrieval (see <a href="https://arxiv.org/abs/2311.14925" target="_blank">SCAN</a> or <a href="https://www.spiedigitallibrary.org/journals/advanced-photonics-nexus/volume-3/issue-05/056005/NeuPh--scalable-and-generalizable-neural-phase-retrieval-with-local/10.1117/1.APN.3.5.056005.full" target="_blank">NeuPh</a>). The novelty here is mostly the observations that we do not need to perform any additional engineering or supply special scaffolding but can rather apply a very direct model and that SIRENs are particularly well suited to the problem, at least in the audio case.
		</p>
		
		<hr class="squiggly-line">

		<h2>Model and loss</h2>

		<p>
			The SIREN paper website has an implementation of SIREN which we use a barebones version of as our model.
		</p>


	<pre><code class="language-py">
class SineLayer(nn.Module):
	def __init__(self, in_features, out_features, bias=True,
				is_first=False, omega_0=30):
		super().__init__()
		self.omega_0 = omega_0
		self.is_first = is_first
		self.linear = nn.Linear(in_features, out_features, bias=bias)
		with torch.no_grad():
			if is_first:
				self.linear.weight.uniform_(-1/in_features, 1/in_features)
			else:
				bound = np.sqrt(6/in_features) / omega_0
				self.linear.weight.uniform_(-bound, bound)

	def forward(self, x):
		return torch.sin(self.omega_0 * self.linear(x))

class Siren(nn.Module):
	def __init__(self, in_features, hidden_features, hidden_layers,
				out_features, first_omega_0=3000, hidden_omega_0=30.):
		super().__init__()
		layers = [SineLayer(in_features, hidden_features,
					is_first=True, omega_0=first_omega_0)]
		for _ in range(hidden_layers):
			layers.append(SineLayer(hidden_features, hidden_features,
						is_first=False, omega_0=hidden_omega_0))
        
		lin = nn.Linear(hidden_features, out_features)
		with torch.no_grad():
			bound = np.sqrt(6/hidden_features) / hidden_omega_0
			lin.weight.uniform_(-bound, bound)
		layers.append(lin)
       
		self.net = nn.Sequential(*layers)

	def forward(self, x):
		return self.net(x)
	</code></pre>
	
	<p>
		Note the first $\omega_0$ hyperparameter which rescales the time variable to be suitable as input to a sinusoid and hence corresponds to a change of variables. The latter $\omega_0$ variables rescale the outputs of the linear layers and are empirically chosen (see the SIREN paper A.1.5 for more details).
	</p>

	<p>
		For our loss we will simply look at the sum of absolute differences between our target and candidate spectrograms. This very general formulation can obviously be applied to other forms of phase retrieval.
	</p>
	<pre><code class="language-py">
class SpectrogramPhaseRetrievalLoss(nn.Module):
	def __init__(self, spec_transform):
		super().__init__()
		self.spec = spec_transform

	def forward(self, pred, target):
		sp_pred   = self.spec(pred.squeeze(1))
		sp_target = self.spec(target.squeeze(1))
		return torch.mean(torch.abs(sp_pred - sp_target))
	</code></pre>

	<p>
		This is essentially all there is to this approach, we optimize with <code>ADAM</code> and use <code>3</code> hidden layers, each of dimensionality <code>256</code>.
	</p>
	
	<hr class="squiggly-line">

	<h2>Baselines</h2>

	<p>
		The Griffin-Lim algorithm is an iterative algorithm which is the most classical solution of the phase retrieval problem, classical enough that PyTorch has a built-in <code>torchaudio.transforms.GriffinLim</code> transform. Our other baseline will be a straightforward gradient descent algorithm on the short-time Fourier transform. This means that we optimize two tensors of the same shape as the spectrogram, corresponding to the real and imaginary parts, and then compute the resulting signal as the inverse short-time Fourier transform. We call this method <i>Complex gradient descent</i>.
	</p>

		<pre><code class="language-py">
# set up target magnitude
mag = spec_transform(y_t.squeeze(1)).abs().detach()

# initialize learnable real & imaginary parts of the STFT
shape = mag.shape

# random phase in [0, 2π)
phi = torch.rand(shape, device=device) * 2 * np.pi

# build real & imaginary parts with correct magnitudes
real_init = mag * torch.cos(phi)
imag_init = mag * torch.sin(phi)

# make them leaf tensors requiring gradients
real_part = real_init.clone().detach().requires_grad_(True)
imag_part = imag_init.clone().detach().requires_grad_(True)

optimizer_complex = optim.Adam([real_part, imag_part], lr=5e-4)
complex_loss_curve = []

# gradient‐descent loop on full complex spectrogram
pbar = tqdm(range(1, NUM_EPOCHS_PHASE+1), desc="Complex Spec GD", ncols=90)
for ep in pbar:
	optimizer_complex.zero_grad()

	# assemble complex spectrogram
	complex_spec = torch.complex(real_part, imag_part)

	# inverse STFT back to time‐domain waveform
	audio_pred = torch.istft(
		complex_spec,
		n_fft=SPECTROGRAM_CONFIG["n_fft"],
		hop_length=SPECTROGRAM_CONFIG["hop_length"],
		win_length=SPECTROGRAM_CONFIG["win_length"],
		window=win,
		length=audio_len
	)

	# compute loss against target spectrogram
	loss = criterion(audio_pred.unsqueeze(1), y_t)
	loss.backward()
	optimizer_complex.step()
	</code></pre>

	
	<hr class="squiggly-line">

	<h2>Results</h2>

	<p>
		In all cases we train the SIREN-based network for <code>2,000</code> epochs and let the complex gradient descent method iterate <code>20,000</code> times. At our empirically decided learning rates this is where the loss has mostly plateaued. The SIREN training takes just under 30 seconds on an RTX 3070 while the complex gradient descent takes just under 90 seconds.
	</p>

	<p>
		First we look at a very simple audio clip with isolated pure tones of varying frequency. In this simple case the SIREN model significantly outperforms the two baselines.
	</p>
	
	<div class="result_group">
		<div class="audio-title">Ground truth</div>
		<audio controls>
			<source src="media/beepboop_ground_truth.mp3" type="audio/mpeg">
		</audio>
		<img src="media/beepboop_ground_truth_spec.png"/>
	</div>
	<div class="result_group">
		<div class="audio-title">SIREN</div>
		<audio controls>
			<source src="media/beepboop_siren.mp3" type="audio/mpeg">
		</audio>
		<img src="media/beepboop_siren_mag.png"/>
	</div>
	<div class="result_group">
		<div class="audio-title">Griffin-Lim</div>
		<audio controls>
			<source src="media/beepboop_griffin_lim.mp3" type="audio/mpeg">
		</audio>
		<img src="media/beepboop_griffin_lim_spec.png"/>
	</div>
	<div class="result_group">
		<div class="audio-title">Complex gradient descent</div>
		<audio controls>
			<source src="media/beepboop_complex_spec.mp3" type="audio/mpeg">
		</audio>
		<img src="media/beepboop_spec_gd_mag.png"/>
	</div>

	<p>
		Next up we look at a more involved audio clip.
	</p>

	<div class="result_group">
		<div class="audio-title">Ground truth</div>
		<audio controls>
			<source src="media/jazz_ground_truth.mp3" type="audio/mpeg">
		</audio>
		<img src="media/jazz_ground_truth_spec.png"/>
	</div>
	<div class="result_group">
		<div class="audio-title">SIREN</div>
		<audio controls>
			<source src="media/jazz_siren.mp3" type="audio/mpeg">
		</audio>
		<img src="media/jazz_siren_mag.png"/>
	</div>
	<div class="result_group">
		<div class="audio-title">Griffin-Lim</div>
		<audio controls>
			<source src="media/jazz_griffin_lim.mp3" type="audio/mpeg">
		</audio>
		<img src="media/jazz_griffin_lim_spec.png"/>
	</div>
	<div class="result_group">
		<div class="audio-title">Complex gradient descent</div>
		<audio controls>
			<source src="media/jazz_complex_spec.mp3" type="audio/mpeg">
		</audio>
		<img src="media/jazz_spec_gd_mag.png"/>
	</div>

	<p>
		Here the defects are more clearly noticeable yet smaller in the case of the SIREN model. A larger model or a shorter audio clip would likely have improved the results further.
	</p>

	<hr class="squiggly-line">

	<h2>Discussion</h2>

	<p>
		We have seen that the SIREN-based network generally performs better than both pure complex gradient descent and the Griffin-Lim algorithm. It is also much faster to train and can better utilize hardware acceleration by virtue of doing more matrix multiplications and fewer fast Fourier transforms behind the scenes. For implementation details see the <a href="https://github.com/SimonHalvdansson/simonhalvdansson.github.io/tree/master/posts/phase-retrieval-inr">GitHub folder</a> for this post.
	</p>

	</section>
	
	<link rel="stylesheet" href="../fancybox.css" />
	<script src="../fancybox.umd.js"></script>

	<script>
		Fancybox.bind("[data-fancybox]", {
			closeButton: false,
		});
	  </script>


</body>
</html>