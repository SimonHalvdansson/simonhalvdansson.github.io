<!DOCTYPE html>
<html lang="en">
<head>
	<title>What do learned positional embeddings look like?</title>

	<meta name="viewport" content="width=device-width, initial-scale=1">
	<meta name="theme-color" content="#FBEDEA" />
	<meta charset="utf-8">

	<link rel="shortcut icon" type="image/x-icon"  href="../../favicon.ico?">
	<link rel="apple-touch-icon" href="../../apple-touch-icon.png">

	<link rel="stylesheet" href="style.css">

	<script>
	window.MathJax = {
	tex: {
		inlineMath: [['$', '$'], ['\\(', '\\)']]
	}
	};
	</script>

	<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

	<link rel="preconnect" href="https://fonts.googleapis.com">
	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>

	<link href="https://fonts.googleapis.com/css2?family=Roboto+Mono:wght@400;500;600;700&display=swap" rel="stylesheet">

	<link rel="stylesheet" href="../atom-one-dark.min.css">
	<script src="../highlight.min.js"></script>

	<script>hljs.highlightAll();</script>

</head>
<body>
	<div class="blog centering" id="back_container">
		<a href="../index.html" class="back"><svg xmlns="http://www.w3.org/2000/svg" width="18" height="14" fill="currentColor" class="bi bi-caret-left" viewBox="0 0 22 18">
			<path fill-rule="evenodd" clip-rule="evenodd" d="M14.0303 7.46967C14.3232 7.76256 14.3232 8.23744 14.0303 8.53033L10.5607 12L14.0303 15.4697C14.3232 15.7626 14.3232 16.2374 14.0303 16.5303C13.7374 16.8232 13.2626 16.8232 12.9697 16.5303L8.96967 12.5303C8.67678 12.2374 8.67678 11.7626 8.96967 11.4697L12.9697 7.46967C13.2626 7.17678 13.7374 7.17678 14.0303 7.46967Z"/>
		  </svg>All posts</a>
	</div>

	<section class="blog centering post">
		<h1>What do learned positional embeddings look like?</h1>
		<div class="subtitle monospace">By Simon Halvdansson&nbsp;|&nbsp;Nov. 2025</div>

		<hr class="squiggly-line"/>

		<p>
			Transformer blocks are famously permutation equivariant, meaning that without intervention, they cannot tell the order of the tokens. One solution to this problem is to learn a tensor of shape <code>(d_model, seq_len)</code> which we add to after embedding our data. It is not clear a-priori what properties these learned positional embeddings should take and it turns out they are substantially different from classical sinusoidal positional embeddings. In this post, we give some background on positional embeddings and investigate what they look like from different directions.
		</p>

		<div class="figure_container" id="main_figure">
			<a data-fancybox="gallery0" href="media/output_self_sim.mp4">
				<video style="width: 50%; float: left;" class="figure" src="media/output_self_sim.mp4" autoplay muted playsinline loop preload="metadata"></video>
			</a>
			<a data-fancybox="gallery0" href="media/positional_self_similarity_sinusoidal.png">
				<img style="width: 50%; float:left" class="figure" src="media/positional_self_similarity_sinusoidal.png"></img>
			</a>
		</div>
		<div class="figure_caption">
			<span class="caption_title">Figure: </span> Cosine self-similarity of learned positional embeddings for <code>seq_len = 256</code> (left) and standard sinusoidal positional mebeddings (right).
		</div>

		<p>
			We will first go over some of the mathematics behind positional embeddings before looking closer at visualizations.
		</p>

		<hr class="squiggly-line"/>

		<h2>Mathematics of additive positional embeddings</h2>

		<p>
			The self-attention mechanism is a self-map which is <b>equivariant</b> to permutations, meaning that if we permute the inputs, the output is permuted in the same way. To see this formally, let $P$ be a permutation matrix and consider
		</p>
		<div class="equation">
			$$
			\begin{align*}
				\operatorname{attention}(PX) &= \operatorname{softmax}\left( \frac{P X W_Q ( P X W_K)^T}{\sqrt{d_k}} \right) PX W_V\\ &= P \operatorname{softmax}\left( \frac{X W_Q ( X W_K)^T}{\sqrt{d_k}} \right) P^T PX W_V = P \operatorname{attention}(X).
			\end{align*}
			$$
		</div>
		<p>
			When we extract a value from the transformer after the final transformer block, we usually read a special <code>[CLS]</code> token or pool all the tokens. Either way, this does not let the transformer react to the order of the tokens in any special way. The original transformer paper used sinusoidal positional embeddings meaning that each token got added a fixed vector which was a sampled sinusoid with increasing frequency along the token dimension. This breaks permutation invariance by adding three new terms to the query-key multiplication in a way which depends on the position. Specifically, with $X$ replaced by $X + E_{\text{emb}}$,
		</p>
		<div class="equation">
			$$
			\begin{align*}
				\operatorname{attention}(X) = \operatorname{softmax}\Bigg( &\frac{X W_Q (X W_K)^T}{\sqrt{d_k}} + \frac{X W_Q (E_{\text{emb}} W_K)^T}{\sqrt{d_k}} \\
				&+ \frac{E_{\text{emb}} W_Q (X W_K)^T}{\sqrt{d_k}} + \frac{E_{\text{emb}} W_Q (E_{\text{emb}} W_K)^T}{\sqrt{d_k}} \Bigg) X W_V.
			\end{align*}
			$$
		</div>
		<p>
			The output of the softmax, the <i>attention weigths</i>, is a <code>seq_len</code> $\times$ <code>seq_len</code> matrix which we take to encode how much the new representation after the self-attention should be influenced by the other tokens. Indeed, $(A V)_i = \sum_{j=1}^L A_{ij} V_j$ where $L$ is <code>seq_len</code>. If this matrix is just the identity, this means that the new representations do not get any extra inter-token dependencies from the self attention (note that $V$ is already a function of $X$ so some inter-token dependency comes built in).
		</p>
		
		<p>
			In the expanded form of the attention above, the first term in the softmax is just the standard attention scores. Meanwhile the second and third terms can be interpreted as their own query-key interactions where we somehow relate the positonal embeddings to the content of the tokens (although the usefulness of these interactions are <a href="https://arxiv.org/pdf/2006.15595" target="_blank">somewhat dubious</a>). The last one is most interesting. If $W_Q W_K^T = I$, then the numerator is precisely the Gram matrix $E_\text{emb} E_{\text{emb}}^T$ which is essentially the cosine similarity from the first figure! Now since $W_Q W_K^T \neq I$, we don't get precisely this but rather something related to it. This provides at least some of the motivation for why we want an off diagonal decay of the self-similarity matrix; representation should be influenced more by tokens close by.
		</p>

		<p>
			Newer ideas in positional embeddings generally treat the problem in a more direct way. A family of ideas going under the name relative positional embeddings have proposed changing the $QK^T$ multiplication in variuos ways, including replacing $E_{\text{emb}} W_Q W_K^T E_{\text{emb}}^T$ by $E_{\text{emb}} U U^T E_{\text{emb}}^T$ with the new matrix $U$ disentangled from the standard query and key projections. One can also directly add a $L \times L$ bias matrix $B$ to the attention scores. We will not look any further at these approaches here but refer to the review in the <a href="https://arxiv.org/abs/2104.09864" target="_blank">RoPE paper</a>.
		</p>

		<hr class="squiggly-line"/>

		<h2>Embedding visualizations</h2>

		<p>
			So far, everything we have said about positional embeddings have been mostly speculation. Meanwhile, transformers and modern machine learning at large is firmly an empirical science. To actually investigate learned embedding, we train a simple <code>89M</code> parameter LLM with <code>n_layers=12</code>, <code>d_model=512</code> and <code>n_head=4</code> on the TinyStories dataset. For more on the model, see the <a href="../llm-training/index.html">earlier post</a>. Throughout traning, we can take snapshots to see how the embeddings evolve.
		</p>

		<p>
			The perhaps most obvious, but also most crude, way to visualize learned embeddings are by plotting the <code>seq_len</code>$\times$<code>d_model</code> matrix which we learn. This is done in the figure below:
		</p>

		<div class="figure_container" style="background-color:white; padding-left: 10px; padding-right: 10px;">
			<a data-fancybox="gallery1" href="media/output_values.mp4">
				<video style="width: 50%; float: left;" class="figure" src="media/output_values.mp4" autoplay muted playsinline loop preload="metadata"></video>
			</a>
			<a data-fancybox="gallery1" href="media/positional_embedding_values_sinusoidal.png">
				<img style="width: 50%; float:left" class="figure" src="media/positional_embedding_values_sinusoidal.png"></img>
			</a>
		</div>
		<div class="figure_caption">
			<span class="caption_title">Figure: </span> Positional embedding matrix over 2 epochs of training, learned on the left and sinusoidal on the right.
		</div>

		<p>
			In the figure we see some structure evolve for the learned positional embeddings, at least along the position axis. This means that for two positions near each other, there is correlation in the values for a certain given dimension, often at least. Meanwhile along a specific position, there is not any apparent correlation as we traverse the embedding dimension. This makes sense because there is no a priori reason that hidden dimension <code>#33</code> should have anything to do with hidden dimension <code>#34</code>. Next up we look closer at three dimensions.
		</p>
		
		<div class="figure_container_small" >
			<a data-fancybox="gallery2" href="media/positional_embeddings_fft_learned_d0_127_255.png">
				<img class="figure" src="media/positional_embeddings_fft_learned_d0_127_255.png"></img>
			</a>
		</div>
		<div class="figure_caption">
			<span class="caption_title">Figure: </span> Learned positional embeddings along the position axis, and their Fourier transforms.
		</div>

		<p>
			There is some notion of drift that can be seen here, albeit a weak one. We see this in the fact that there is marginally more power in the lower frequencies.
		</p>

		<p>
			If we compare this to sinusoidal positional embeddings, the difference is stark and the patterns are much smoother.
		</p>

		<div class="figure_container_small" >
			<a data-fancybox="gallery2" href="media/positional_embeddings_fft_sinusoidal_d0_127_255.png">
				<img class="figure" src="media/positional_embeddings_fft_sinusoidal_d0_127_255.png"></img>
			</a>
		</div>
		<div class="figure_caption">
			<span class="caption_title">Figure: </span> Sinusoidal positional embeddings along the position axis, and their Fourier transforms.
		</div>

		<h3>Cosine self-similarity</h3>

		<p>
			On the topic of inner products, self-similarity is a common way to visualize the inter-dependence between positional embeddings and is the method we used for the figure at the top of the post, and below. Formally, the cosine self-similarity between two vectors $u,v$ is the normalized inner product
		</p>
		<div class="equation">
			$$
			\frac{\langle u,v\rangle}{ \Vert u \Vert \Vert v \Vert}
			$$
		</div>
		<p>
			which we know is always normalized in the $[-1, 1]$ range. In the figure, we see how the off-diagonal decay gets weaker as the embedding settle in.
		</p>

		<div class="figure_container" style="background-color:white; padding-left: 10px; padding-right: 10px;">
			<a data-fancybox="gallery3" href="media/output_self_sim.mp4">
				<video style="width: 50%; float: left;" class="figure" src="media/output_self_sim.mp4" autoplay muted playsinline loop preload="metadata"></video>
			</a>
			<a data-fancybox="gallery3" href="media/positional_self_similarity_sinusoidal.png">
				<img style="width: 50%; float:left" class="figure" src="media/positional_self_similarity_sinusoidal.png"></img>
			</a>
		</div>
		<div class="figure_caption">
			<span class="caption_title">Figure: </span> Cosine self-similarity of learned positional embeddings for <code>seq_len = 256</code> (left) and standard sinusoidal positional mebeddings (right).
		</div>

		<p>
			It can be shown that not-too-fast off-diagonal decay of this Gram matrix is equivalent to the uniform continuity of vectors in the index variable. Indeed, let $(u_n)_n$ be a sequence of normalized vectors (in a real inner product space), then $\Vert u_n - u_m \Vert^2 = 2(1-\langle u_n, u_m \rangle)$ and so
		</p>
		<div class="equation">
			$$
			\begin{align*}
				\langle u_n, u_m \rangle \geq 1-C|n - m| &\iff 1 - \frac{1}{2}\Vert u_n - u_m \Vert^2 \geq 1 - C|n - m|\\
				&\iff \Vert u_n - u_m \Vert^2 \leq 2C |n-m|.
			\end{align*}
			$$
		</div>

		<p>
			Note that for us, the index variable is the embedding dimension .
		</p>

		<p>
			Lastly, inspired by the fact that order does not matter for learned embeddings, we look at the density 

			TODO: Should we look at density for a fixed embedding dimension instead?
		</p>


		<div class="figure_container_small">
			<a data-fancybox="gallery4" href="media/output_density.mp4">
				<video class="figure" src="media/output_density.mp4" autoplay muted playsinline loop preload="metadata"></video>
			</a>
		</div>
		<div class="figure_caption">
			<span class="caption_title">Figure: </span> TODO!
		</div>

		<div class="figure_container_small" >
			<a data-fancybox="gallery2" href="media/positional_embedding_density_sinusoidal.png">
				<img class="figure" src="media/positional_embedding_density_sinusoidal.png"></img>
			</a>
		</div>
		<div class="figure_caption">
			<span class="caption_title">Figure: </span> Sinusoidal positional embeddings along the position axis, and their Fourier transforms.
		</div>
		

	</section>
	
	<link rel="stylesheet" href="../fancybox.css" />
	<script src="../fancybox.umd.js"></script>


	<script>
		Fancybox.bind("[data-fancybox]", {
			closeButton: false,
		});
	  </script>


</body>
</html>