<!DOCTYPE html>
<html>
<head>
	<title>Gabor features let networks learn time-frequency structured functions in low dimensional domains</title>

	<meta name="viewport" content="width=device-width, initial-scale=1">
	<meta name="theme-color" content="#FBEDEA" />

	<link rel="shortcut icon" type="image/x-icon"  href="../../favicon.ico?">
	<link rel="apple-touch-icon" href="../../apple-touch-icon.png">

	<link rel="stylesheet" href="style.css">

	<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

	<link rel="preconnect" href="https://fonts.googleapis.com">
	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>

	<link href="https://fonts.googleapis.com/css2?family=Roboto+Mono:wght@400;500;600;700&display=swap" rel="stylesheet">

	<link rel="stylesheet" href="../atom-one-dark.min.css">
	<script src="../highlight.min.js"></script>

	<script>hljs.highlightAll();</script>

	<style>

        #vises canvas {
            border: 1px solid black;
            margin: 10px;
            width: 256px;
            height: 256px;
        }
        #canvases {
            display: flex;
            flex-wrap: wrap;
            justify-content: center;
        }
		#vises {
			font-size: 13px;
			color: #000;
			font-family: 'Roboto mono', Courier, monospace;
		}

        #controls {
            margin: 20px 0;
            display: flex;
            flex-direction: column;
            align-items: center;
        }
		#windowSizeControl {
            margin: 10px 0;
        }
        #scalingOptions label, #windowFunctionOptions label {
            margin-right: 15px;
            cursor: pointer;
        }
        /* Updated Positioning for the original image and its overlay using CSS Grid */
        #originalContainer {
            display: grid;
            grid-template-areas:
                "label"
                "canvas";
            grid-template-columns: 1fr;
            grid-template-rows: auto 1fr;
            position: relative;
        }
        #originalContainer p {
            grid-area: label;
        }
        #originalCanvas {
            grid-area: canvas;
        }
        #originalOverlayCanvas {
            grid-area: canvas;
            pointer-events: none; /* Allows mouse events to pass through */
            z-index: 1; /* Ensure overlay is on top */
        }
        /* Slider styling */
        #windowSizeSlider {
            width: 200px;
        }
        #windowSizeValue {
            margin-left: 10px;
            font-weight: bold;
        }
		.equation {
			text-align: center;
			max-width: 100%;
			overflow-x: auto;
			overflow-y: hidden;
			padding-left: 16px;
			padding-right: 16px;
			padding-bottom: 4px;
		}

		/* Enhanced Slider Styling - Material Design */
	#windowSizeControl {
		margin: 10px 0;
		display: flex;
		align-items: center;
	}

	#windowSizeControl label {
		margin-right: 15px;
		font-weight: 500;
	}

	#windowSizeSlider {
		-webkit-appearance: none;
		width: 200px;
		height: 4px;
		background: #d3d3d3;
		border-radius: 2px;
		outline: none;
		margin: 0 10px;
		transition: background 0.3s;
	}

	#windowSizeSlider:hover {
		background: #bdbdbd;
	}

	/* Webkit */
	#windowSizeSlider::-webkit-slider-thumb {
		-webkit-appearance: none;
		appearance: none;
		width: 16px;
		height: 16px;
		background: #6200ee;
		border: 2px solid #ffffff;
		border-radius: 50%;
		cursor: pointer;
		transition: background 0.3s, box-shadow 0.3s;
		box-shadow: 0 2px 4px rgba(0,0,0,0.3);
	}

	#windowSizeSlider::-webkit-slider-thumb:hover {
		background: #3700b3;
	}

	/* Mozilla */
	#windowSizeSlider::-moz-range-thumb {
		width: 16px;
		height: 16px;
		background: #6200ee;
		border: 2px solid #ffffff;
		border-radius: 50%;
		cursor: pointer;
		transition: background 0.3s, box-shadow 0.3s;
		box-shadow: 0 2px 4px rgba(0,0,0,0.3);
	}

	#windowSizeSlider::-moz-range-thumb:hover {
		background: #3700b3;
	}

	/* Thumbnail Gallery */
	#thumbnails {
		display: flex;
		flex-wrap: wrap;
		justify-content: center;
		margin-top: 20px;
	}

	.thumbnail {
		width: 50px;
		height: 50px;
		object-fit: cover;
		margin: 5px;
		border: 2px solid transparent;
		border-radius: 4px;
		cursor: pointer;
		transition: border-color 0.3s, transform 0.3s;
	}

	.thumbnail:hover {
		border-color: #6200ee; /* Material Design primary color */
		transform: scale(1.05);
	}

	.thumbnail.selected {
		border-color: #3700b3;
		transform: scale(1.05);
	}

    </style>


</head>
<body>
	<div class="blog centering" id="back_container">
		<a href="https://simonhalvdansson.github.io/posts/index.html" class="back"><svg xmlns="http://www.w3.org/2000/svg" width="18" height="14" fill="currentColor" class="bi bi-caret-left" viewBox="0 0 22 18">
			<path fill-rule="evenodd" clip-rule="evenodd" d="M14.0303 7.46967C14.3232 7.76256 14.3232 8.23744 14.0303 8.53033L10.5607 12L14.0303 15.4697C14.3232 15.7626 14.3232 16.2374 14.0303 16.5303C13.7374 16.8232 13.2626 16.8232 12.9697 16.5303L8.96967 12.5303C8.67678 12.2374 8.67678 11.7626 8.96967 11.4697L12.9697 7.46967C13.2626 7.17678 13.7374 7.17678 14.0303 7.46967Z"/>
		  </svg>All posts</a>
	</div>

	<section class="blog centering post">
		<h1>Experiments with Gabor features for learning time-frequency structured functions in low dimensional domains</h1>
		<div class="subtitle monospace">By Simon Halvdansson&nbsp;|&nbsp;Mar. 2025</div>

		<hr class="squiggly-line"/>

		<p>
			Neural networks are well known to be <a href="https://en.wikipedia.org/wiki/Universal_approximation_theorem">universal function approximators</a>, meaning roughly that for any not-too-badly-behaved function \( f \) and \(\varepsilon > 0\), we can find a neural network \( \mathcal{N} \) such that \( \Vert f - \mathcal{N} \Vert < \varepsilon\). However, the neural network \( \mathcal{N} \) might have to be unfeasibly large to approximate some functions. To mitigate this problem, we can first apply a <i>feature mapping</i> where instead of applying the neural network to the input coordinate \( \boldsymbol{x} \) directly, we apply some preprocessing \(F\) to \( \boldsymbol{x} \) and approximate the target function by \( \mathcal{N}(F(\boldsymbol{x}))\) instead. Specifically, we will be interested in feature mappings \( F \) which treats position (time) and frequency separately.
		</p>

		<div class="figure_container">
			<a style="width: 33.3%; float:left; border-bottom:none" data-fancybox="gallery0" href="media/image2_naive_progress.png">
				<img class="figure"  src="media/image2_naive_progress.gif"></img>
			</a>
			<a style="width: 33.3%; float:left; border-bottom:none" data-fancybox="gallery1" href="media/image2_opt_fourier_progress.png">
				<img class="figure"  src="media/image2_opt_fourier_progress.gif"></img>
			</a>
			<a style="width: 33.3%; float:left; border-bottom:none" data-fancybox="gallery2" href="media/image2_opt_gabor_progress.png">
				<img class="figure"  src="media/image2_opt_gabor_progress.gif"></img>
			</a>
		</div>
		<div class="figure_caption">
			<span class="caption_title">Figure:</span> Snapshots from the training process of approximating an RGB image for identity \( F(\boldsymbol{x}) = \boldsymbol{x} \) (left), Fourier \( F(\boldsymbol{x}) = (\cos(\boldsymbol{a_1} \cdot \boldsymbol{x}),\, \sin(\boldsymbol{b_1} \cdot \boldsymbol{x}),\, \dots) \) (middle) and Gabor \( F(\boldsymbol{x}) = (e^{- \sigma_1 \Vert \boldsymbol{c_1} - \boldsymbol{x} \Vert^2}\cos(\boldsymbol{a_1} \cdot \boldsymbol{x}),\, e^{- \sigma_1 \Vert \boldsymbol{c_1} - \boldsymbol{x} \Vert^2}\sin(\boldsymbol{b_1} \cdot \boldsymbol{x}),\, \dots) \) (right) feature mappings.
		</div>

		<p>
			A natural question to ask is <b>why</b> one might wish to approximate a function representing an image, a signal, a video or a 3D model by a neural network. After all, the "standard" setting of a neural network is to map <i>high</i> dimensional data into either a few dimensions (classification, segmentation, object detection) or many dimensions (style transfer, denoising, point cloud completion). The two motivating settings closest to my heart is physics-informed neural networks (PINNs) and neural texture compression. In the former, we learn a function which should minimize a PDE loss functional and in the latter, made famous by the recent <a href="https://developer.nvidia.com/blog/nvidia-rtx-neural-rendering-introduces-next-era-of-ai-powered-graphics-innovation/" target="_blank">NVIDIA launch</a>, we compress textures by expressing them as a neural network. In both of these cases, it is crucial that our networks are expressive in the sense that we can output functions without using too many parameters or epochs of training.
		</p>

		<p>
			The most influential paper on this topic is the seminal 2020 paper <a href="https://arxiv.org/abs/2006.10739" target="_blank">Fourier Features Let Networks Learn High Frequency Functions in Low Dimensional Domains</a>. In this post, we will go over some of the empirical insights from that paper and discuss an extension to Gabor atoms.
		</p>

		<hr class="squiggly-line"/>
		<h2>Fourier features</h2>

		<p>
			To illustrate why 
		</p>


	</section>
	
	<link rel="stylesheet" href="../fancybox.css" />
	<script src="../fancybox.umd.js"></script>

	<script>
		Fancybox.bind("[data-fancybox]", {
			closeButton: false,
		});
	  </script>


</body>
</html>